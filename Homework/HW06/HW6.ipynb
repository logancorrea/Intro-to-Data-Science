{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science â€“ Homework 6\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "Due: Friday, March 01 2024, 11:59pm.\n",
    "\n",
    "In Part 1 of this homework you will scrape github repositories and organize the information in a Pandas dataframe. In Part 2, you will use linear regression to gain meaningful insights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Data\n",
    "First Name: Logan\n",
    "<br>\n",
    "Last Name: Correa\n",
    "<br>\n",
    "E-mail: u1094034@umail.utah.edu\n",
    "<br>\n",
    "UID: u1094034\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup \n",
    "from bs4 import BeautifulSoup\n",
    "# you can use either of these libraries to get html from a website\n",
    "import time\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (10, 6) \n",
    "# where the data is stored\n",
    "DATA_PATH = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scrape Github Repository List using BeautifulSoup\n",
    "In this part you will explore Github repositories, specifically the 100 most-starred repositories. You are going to scrape data from a snapshot of [this repository list](https://github.com/search?o=desc&q=stars%3A%3E1&s=stars&type=Repositories)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Check whether you are permitted to scrape the data\n",
    "Before you start to scrape any website you should go through the terms of service and policy documents of the website. Almost all websites post conditions to use their data. Check the terms of [https://github.com/](https://github.com/) (see the tiny \"terms\" link at the bottom of the page) to see whether the site permits you to scrape their data or not. Are you sure you are allowed to scrape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your solution:**\n",
    "\n",
    "Scraping is allowed for researchers as long as published works are open access and the information is not used for spamming purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 Load the Data\n",
    "\n",
    "To avoid any problems with GitHub blocking us from downloading the data many times, we have downloaded and saved a snapshot of the html files for you in the [data](data) folder. Note that the data folder is not completely consistent with what you see on the web â€“ we've made a few patches to the data that makes your task here easier and this data represents a snapshot in time. You will be treating the data folder as your website to be scraped. The path to data folder is stored in `DATA_PATH` variable.\n",
    "\n",
    "In the data folder you will find first 10 pages of highly starred repositories saved as `searchPage1.html`,`searchPage2.html`,`searchPage3.html` ... `searchPage10.html`\n",
    "\n",
    "Check out page 10 if you want to see what happens if you scrape too quickly ðŸ˜‰. \n",
    "\n",
    "Now read these html files in python and create a soup object. This is a two step process:\n",
    " * Read the text in the html files\n",
    " * Create the soup from the files that you've read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read html files and create soup files\n",
    "html_pages = []\n",
    "for files in os.listdir(DATA_PATH):\n",
    "    if files.endswith(\".html\"):\n",
    "        full_path = os.path.join(DATA_PATH, files)\n",
    "        with open(full_path, 'r') as f:\n",
    "            html_pages.append(f.read())\n",
    "\n",
    "SearchPage_soup = []\n",
    "for page in html_pages:\n",
    "    SearchPage_soup.append(BeautifulSoup(page, 'html.parser'))\n",
    "\n",
    "len(SearchPage_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data\n",
    "\n",
    "Extract the following data for each repository, and create a Pandas Dataframe with a row for each repository and a column for each of these datums. \n",
    "\n",
    "+ The name of the repository\n",
    "+ The primary language (there are multiple or none, if multiple, use the first one, if none, use \"none\")\n",
    "+ The number of watches\n",
    "+ The number of stars\n",
    "+ The number of forks\n",
    "+ The number of issues\n",
    "+ Number of commits\n",
    "+ Number of contributors\n",
    "+ Number of pull requests, and\n",
    "+ Number of top level folders in the file list.\n",
    "\n",
    "Here's an example for one repository, `jackfrued/Python-100-Days,` in our dataset: \n",
    "```python\n",
    "{'name': 'Python-100-Days',\n",
    "'language': 'Jupyter Notebook',\n",
    "'watches': '4822',\n",
    "'stars': '78068',\n",
    "'forks': '30979',\n",
    "'issues': 224,\n",
    "'commits': 296,\n",
    "'contributors': 12,\n",
    "'pull_requests':85,\n",
    "'folders': 14\n",
    "}\n",
    "```\n",
    "\n",
    "### Task 1.3 Extract repository URLs\n",
    "\n",
    "If you look at the results of the 100 most-starred repositories [(this list)](https://github.com/search?o=desc&q=stars%3A%3E1&s=stars&type=Repositories), you will notice that all the information we want to extract for each repository is not in that list. This information is in the repositoryâ€™s individual web page, for example [996icu](https://github.com/996icu/996.ICU). \n",
    "\n",
    "Therefore, you will first have to extract links of each repository from the soup you scraped earlier. When you extract the link for the repository, it will be a path to the stored HTML page for the repository. You will use this path to read the file and extract the above information.\n",
    "\n",
    "Refer to the scraping lecture for details on how to do this. We recommend you use the web inspector to identify the relevant structures.\n",
    "\n",
    "Example of a link that you need to extract - 996icu/996.ICU.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "['netdata/netdata', 'tonsky/FiraCode', 'denoland/deno', 'h5bp/html5-boilerplate', 'ElemeFE/element', 'adam-p/markdown-here', 'h5bp/Front-end-Developer-Interview-Questions', 'resume/resume.github.com', 'josephmisiti/awesome-machine-learning', 'lodash/lodash', 'angular/angular.js', 'puppeteer/puppeteer', 'mrdoob/three.js', 'microsoft/TypeScript', 'angular/angular', 'microsoft/terminal', 'laravel/laravel', 'moby/moby', 'ant-design/ant-design', 'iluwatar/java-design-patterns', 'ossu/computer-science', '30-seconds/30-seconds-of-code', 'mui-org/material-ui', 'jquery/jquery', 'webpack/webpack', 'reduxjs/redux', 'nvbn/thefuck', 'vuejs/awesome-vue', 'avelino/awesome-go', 'atom/atom', 'apple/swift', 'hakimel/reveal.js', 'MisterBooo/LeetCodeAnimation', 'PanJiaChen/vue-element-admin', 'pallets/flask', 'socketio/socket.io', 'expressjs/express', 'Semantic-Org/Semantic-UI', 'shadowsocks/shadowsocks-windows', 'chartjs/Chart.js', 'jwasham/coding-interview-university', 'kamranahmedse/developer-roadmap', 'github/gitignore', 'airbnb/javascript', 'microsoft/vscode', 'CyC2018/CS-Notes', 'd3/d3', 'flutter/flutter', 'torvalds/linux', 'facebook/react-native', 'nodejs/node', 'TheAlgorithms/Python', 'daneden/animate.css', 'kubernetes/kubernetes', 'justjavac/free-programming-books-zh_CN', 'FortAwesome/Font-Awesome', 'trekhleb/javascript-algorithms', 'tensorflow/models', 'ytdl-org/youtube-dl', 'danistefanovic/build-your-own-x', 'freeCodeCamp/freeCodeCamp', '996icu/996.ICU', 'vuejs/vue', 'facebook/react', 'tensorflow/tensorflow', 'twbs/bootstrap', 'EbookFoundation/free-programming-books', 'sindresorhus/awesome', 'getify/You-Dont-Know-JS', 'ohmyzsh/ohmyzsh', 'donnemartin/system-design-primer', 'electron/electron', 'vinta/awesome-python', 'jackfrued/Python-100-Days', 'facebook/create-react-app', 'public-apis/public-apis', 'axios/axios', 'golang/go', 'Snailclimb/JavaGuide', 'jlevy/the-art-of-command-line', 'django/django', 'elastic/elasticsearch', 'keras-team/keras', 'jakubroztocil/httpie', 'storybookjs/storybook', 'typicode/json-server', 'chrislgarry/Apollo-11', 'spring-projects/spring-boot', 'rails/rails', 'zeit/next.js']\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract all 'div' elements with class \"mt-n1\" (these classes contain individual repositories) from each item in SearchPage_soup.\n",
    "repositories = []\n",
    "\n",
    "for i in range(0, len(SearchPage_soup)):\n",
    "    mtn1 = SearchPage_soup[i].find_all('div', class_=\"mt-n1\")\n",
    "    for item in mtn1:\n",
    "        repo_links = item.find('a', class_=\"v-align-middle\")\n",
    "        repositories.extend(repo_links.text.split())\n",
    "\n",
    "print(len(repositories))\n",
    "print(repositories)\n",
    "\n",
    "repositories = [repo_path + '.html' for repo_path in repositories]\n",
    "\n",
    "htmls = []\n",
    "\n",
    "for repo_path in repositories:\n",
    "    # Construct the full path to the .html file\n",
    "    full_file_path = os.path.join(DATA_PATH, repo_path)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(full_file_path):\n",
    "        # Open and read the file\n",
    "        with open(full_file_path, 'r') as file:\n",
    "            html_content = file.read()\n",
    "            htmls.append(html_content)\n",
    "    else:\n",
    "        print(\"File not found:\", full_file_path)\n",
    "\n",
    "print(len(htmls))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "soups = []\n",
    "for page in htmls:\n",
    "    soups.append(BeautifulSoup(page, 'html.parser'))\n",
    "\n",
    "print(len(soups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "htmls = []\n",
    "for root, dirs, files in os.walk(DATA_PATH):\n",
    "    for file in files:\n",
    "        if file.endswith(\".html\") and not any(file == f\"searchPage{i}.html\" for i in range(1, 11)):\n",
    "            full_path = os.path.join(root, file)\n",
    "            with open(full_path, 'r') as f:\n",
    "                htmls.append(f.read())\n",
    "\n",
    "soups = []\n",
    "for page in htmls:\n",
    "    soups.append(BeautifulSoup(page, 'html.parser'))\n",
    "\n",
    "print(len(soups))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository Names\n",
    "repo_names = []\n",
    "\n",
    "for soup in soups:\n",
    "    element = soup.find(\"a\", attrs={\"data-pjax\": \"#js-repo-pjax-container\"})\n",
    "    repo_names.append(element.text)\n",
    "\n",
    "# Languages\n",
    "lang = []\n",
    "\n",
    "for soup in soups:\n",
    "    element = soup.find(\"span\", class_=\"language-color\")\n",
    "    if element:\n",
    "        lang.append(element.text)\n",
    "    else:\n",
    "        lang.append(\"None\")\n",
    "\n",
    "# Number of watchers\n",
    "watchers = []\n",
    "\n",
    "for soup in soups:\n",
    "    element = soup.find(\"a\", class_=\"social-count\", href=lambda x: x and \"/watchers\" in x)\n",
    "    watchers.append(element.text.strip())\n",
    "\n",
    "# Number of stars\n",
    "stars = []\n",
    "\n",
    "for soup in soups:\n",
    "    element = soup.find('a', class_=\"social-count js-social-count\", href=lambda x: x and \"/stargazers\" in x)\n",
    "    stars.append(element.text.strip())\n",
    "\n",
    "# Number of forks\n",
    "forks = []\n",
    "\n",
    "for soup in soups:\n",
    "    soup.find('a', class_=\"social-count\", href=lambda x: x and \"/members\" in x)\n",
    "    forks.append(element.text.strip())\n",
    "\n",
    "# Number of Issues\n",
    "issues = []\n",
    "\n",
    "for soup in soups:\n",
    "    issue_link = soup.find('a', href=lambda x: x and '/issues' in x)\n",
    "    if issue_link:\n",
    "        issue_count = issue_link.find('span', class_='Counter')\n",
    "        if issue_count:\n",
    "            issues.append(issue_count.text.strip())\n",
    "        else:\n",
    "            issues.append(\"No Field Found\")\n",
    "    else:\n",
    "        issues.append(\"No Field Found\")\n",
    "\n",
    "# Number of commits\n",
    "commits = []\n",
    "\n",
    "for soup in soups:\n",
    "    commit_link = soup.find('a', href=lambda x: x and '/commits/master' in x)\n",
    "    if commit_link:\n",
    "        commit_count = commit_link.find('span', class_='num text-emphasized')\n",
    "        if commit_count:\n",
    "            commits.append(commit_count.text.strip())\n",
    "        else:\n",
    "            commits.append(\"No Field Found\")\n",
    "    else:\n",
    "        commits.append(\"No Field Found\")   \n",
    "\n",
    "#  Number of contributors\n",
    "contributors = []\n",
    "\n",
    "for soup in soups:\n",
    "    contributors_link = soup.find('a', href=lambda x: x and '/contributors' in x)\n",
    "    contributors_count = contributors_link.find('span', class_='num text-emphasized')\n",
    "    contributors.append(contributors_count.text.strip())\n",
    "\n",
    "\n",
    "# Number of pulls\n",
    "pulls = []\n",
    "\n",
    "for soup in soups:\n",
    "    pulls_link = soup.find('a', href=lambda x: x and '/pulls' in x)\n",
    "    pulls_count = pulls_link.find('span', class_='Counter')\n",
    "    pulls.append(pulls_count.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\"Name\": repo_names, \"Language\": lang, \"Watchers\": watchers, \"Stars\": stars, \"Forks\": forks, \"Issues\": issues, \"Commits\": commits, \"Contributors\": contributors, \"Pull_Requests\": pulls}\n",
    "project_info = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4 Extracting required information\n",
    "\n",
    "Once you have extracted links for each repository, you can start parsing those HTML pages using BeautifulSoup and extract all the required information.\n",
    "\n",
    "**Note**: There are few repositories which do not contain 'issues' field (such as 996icu/996.ICU.html). Therefore, write your code such that it handles this condition as well.\n",
    "\n",
    "**Save the dataframe you created to a new file project_info.csv and include this in your submission.** This separate file will also be graded and is required to earn points.\n",
    "\n",
    "You also need to make sure that you reformat all numerical columns to be integer data. You can do that either as you parse, or when you have a dataframe with strings.\n",
    "\n",
    "Note that there is one repository flagged as having infinite contributers (the Linux kernel). We'll assume that it in fact has 15600 contributors (an estimate based on a Google search at the time of download)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_convert(value):\n",
    "    value_str = str(value)\n",
    "    if 'k' in value_str:\n",
    "        # Remove 'k' and convert to float, then multiply by 1000\n",
    "        return int(float(value.replace('k', '')) * 1000)\n",
    "    else:\n",
    "        return int(value)\n",
    "\n",
    "# Apply the conversion function to the 'Counts' column\n",
    "project_info['Watchers'] = project_info['Watchers'].apply(int_convert)\n",
    "project_info['Stars'] = project_info['Stars'].apply(int_convert)\n",
    "project_info['Forks'] = project_info['Forks'].apply(int_convert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Language</th>\n",
       "      <th>Watchers</th>\n",
       "      <th>Stars</th>\n",
       "      <th>Forks</th>\n",
       "      <th>Issues</th>\n",
       "      <th>Commits</th>\n",
       "      <th>Contributors</th>\n",
       "      <th>Pull_Requests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>netdata</td>\n",
       "      <td>C</td>\n",
       "      <td>1400</td>\n",
       "      <td>44800</td>\n",
       "      <td>45000</td>\n",
       "      <td>680</td>\n",
       "      <td>9,816</td>\n",
       "      <td>323</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FiraCode</td>\n",
       "      <td>Clojure</td>\n",
       "      <td>720</td>\n",
       "      <td>44400</td>\n",
       "      <td>45000</td>\n",
       "      <td>222</td>\n",
       "      <td>364</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deno</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>1500</td>\n",
       "      <td>44400</td>\n",
       "      <td>45000</td>\n",
       "      <td>320</td>\n",
       "      <td>2,786</td>\n",
       "      <td>220</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>html5-boilerplate</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>2600</td>\n",
       "      <td>44000</td>\n",
       "      <td>45000</td>\n",
       "      <td>2</td>\n",
       "      <td>1,778</td>\n",
       "      <td>231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>element</td>\n",
       "      <td>Vue</td>\n",
       "      <td>1400</td>\n",
       "      <td>43800</td>\n",
       "      <td>45000</td>\n",
       "      <td>1,235</td>\n",
       "      <td>No Field Found</td>\n",
       "      <td>501</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>json-server</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>973</td>\n",
       "      <td>45500</td>\n",
       "      <td>45000</td>\n",
       "      <td>448</td>\n",
       "      <td>789</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Apollo-11</td>\n",
       "      <td>Assembly</td>\n",
       "      <td>1300</td>\n",
       "      <td>45400</td>\n",
       "      <td>45000</td>\n",
       "      <td>14</td>\n",
       "      <td>354</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>spring-boot</td>\n",
       "      <td>Java</td>\n",
       "      <td>3400</td>\n",
       "      <td>45400</td>\n",
       "      <td>45000</td>\n",
       "      <td>423</td>\n",
       "      <td>25,192</td>\n",
       "      <td>651</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>rails</td>\n",
       "      <td>Ruby</td>\n",
       "      <td>2600</td>\n",
       "      <td>45000</td>\n",
       "      <td>45000</td>\n",
       "      <td>404</td>\n",
       "      <td>75,943</td>\n",
       "      <td>3,973</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>next.js</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>846</td>\n",
       "      <td>45000</td>\n",
       "      <td>45000</td>\n",
       "      <td>272</td>\n",
       "      <td>No Field Found</td>\n",
       "      <td>917</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name    Language  Watchers  Stars  Forks Issues  \\\n",
       "0             netdata           C      1400  44800  45000    680   \n",
       "1            FiraCode     Clojure       720  44400  45000    222   \n",
       "2                deno  TypeScript      1500  44400  45000    320   \n",
       "3   html5-boilerplate  JavaScript      2600  44000  45000      2   \n",
       "4             element         Vue      1400  43800  45000  1,235   \n",
       "..                ...         ...       ...    ...    ...    ...   \n",
       "85        json-server  JavaScript       973  45500  45000    448   \n",
       "86          Apollo-11    Assembly      1300  45400  45000     14   \n",
       "87        spring-boot        Java      3400  45400  45000    423   \n",
       "88              rails        Ruby      2600  45000  45000    404   \n",
       "89            next.js  JavaScript       846  45000  45000    272   \n",
       "\n",
       "           Commits Contributors Pull_Requests  \n",
       "0            9,816          323            37  \n",
       "1              364           70             2  \n",
       "2            2,786          220            32  \n",
       "3            1,778          231             1  \n",
       "4   No Field Found          501           294  \n",
       "..             ...          ...           ...  \n",
       "85             789           60            60  \n",
       "86             354          103             4  \n",
       "87          25,192          651            27  \n",
       "88          75,943        3,973           239  \n",
       "89  No Field Found          917            30  \n",
       "\n",
       "[90 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(project_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_info.to_csv('project_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analyzing the repository data\n",
    "\n",
    "In this part, you will analyze the data collected in Part 1 using regression tools. The goal is to identify properties that make a repository popular. \n",
    "\n",
    "First, load the `project_info.csv` file in again. **We need you to do this so that we can run your code below without having to run your scraping code, which can be slow.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Language</th>\n",
       "      <th>Watchers</th>\n",
       "      <th>Stars</th>\n",
       "      <th>Forks</th>\n",
       "      <th>Issues</th>\n",
       "      <th>Commits</th>\n",
       "      <th>Contributors</th>\n",
       "      <th>Pull_Requests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>netdata</td>\n",
       "      <td>C</td>\n",
       "      <td>1400</td>\n",
       "      <td>44800</td>\n",
       "      <td>82700</td>\n",
       "      <td>680</td>\n",
       "      <td>9,816</td>\n",
       "      <td>323</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>computer-science</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3400</td>\n",
       "      <td>54900</td>\n",
       "      <td>82700</td>\n",
       "      <td>8</td>\n",
       "      <td>No Field Found</td>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the-art-of-command-line</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2200</td>\n",
       "      <td>68200</td>\n",
       "      <td>82700</td>\n",
       "      <td>78</td>\n",
       "      <td>1,185</td>\n",
       "      <td>150</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>awesome-go</td>\n",
       "      <td>Go</td>\n",
       "      <td>2600</td>\n",
       "      <td>51900</td>\n",
       "      <td>82700</td>\n",
       "      <td>8</td>\n",
       "      <td>3,328</td>\n",
       "      <td>1,195</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>free-programming-books</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8600</td>\n",
       "      <td>137000</td>\n",
       "      <td>82700</td>\n",
       "      <td>19</td>\n",
       "      <td>5,200</td>\n",
       "      <td>1,096</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name Language  Watchers   Stars  Forks Issues  \\\n",
       "0                  netdata        C      1400   44800  82700    680   \n",
       "1         computer-science      NaN      3400   54900  82700      8   \n",
       "2  the-art-of-command-line      NaN      2200   68200  82700     78   \n",
       "3               awesome-go       Go      2600   51900  82700      8   \n",
       "4   free-programming-books      NaN      8600  137000  82700     19   \n",
       "\n",
       "          Commits Contributors  Pull_Requests  \n",
       "0           9,816          323             37  \n",
       "1  No Field Found           91              1  \n",
       "2           1,185          150             89  \n",
       "3           3,328        1,195             19  \n",
       "4           5,200        1,096             16  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_info = pd.read_csv('project_info.csv')\n",
    "project_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 Describe the data\n",
    "\n",
    "+ Get an overview of the data using the describe function.\n",
    "+ Compute the correlation matrix, visualize it with a heat map.\n",
    "+ Visualize the correlations by making a scatterplot matrix.\n",
    "+ Interprete what you see.\n",
    "\n",
    "You can re-use code from your previous homework here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Linear regression\n",
    "\n",
    "1. Use linear regression to try to predict the number of Stars based on Forks, Pull Requests, and Number of Folders. Explain why this is not a very good model by discussing the R-squared , F-statistic p-value, and coefficient  p-values. \n",
    "+ Develop another model which is better. Explain why it is better and interpret your results. Hint: try using other variables such as Watches and/or Contributors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your interpretation:** TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
