{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science â€“ Homework 6\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "Due: Friday, March 01 2024, 11:59pm.\n",
    "\n",
    "In Part 1 of this homework you will scrape github repositories and organize the information in a Pandas dataframe. In Part 2, you will use linear regression to gain meaningful insights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Data\n",
    "First Name: Logan\n",
    "<br>\n",
    "Last Name: Correa\n",
    "<br>\n",
    "E-mail: u1094034@umail.utah.edu\n",
    "<br>\n",
    "UID: u1094034\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup \n",
    "from bs4 import BeautifulSoup\n",
    "# you can use either of these libraries to get html from a website\n",
    "import time\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (10, 6) \n",
    "# where the data is stored\n",
    "DATA_PATH = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scrape Github Repository List using BeautifulSoup\n",
    "In this part you will explore Github repositories, specifically the 100 most-starred repositories. You are going to scrape data from a snapshot of [this repository list](https://github.com/search?o=desc&q=stars%3A%3E1&s=stars&type=Repositories)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Check whether you are permitted to scrape the data\n",
    "Before you start to scrape any website you should go through the terms of service and policy documents of the website. Almost all websites post conditions to use their data. Check the terms of [https://github.com/](https://github.com/) (see the tiny \"terms\" link at the bottom of the page) to see whether the site permits you to scrape their data or not. Are you sure you are allowed to scrape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your solution:**\n",
    "\n",
    "Scraping is allowed for researchers as long as published works are open access and the information is not used for spamming purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 Load the Data\n",
    "\n",
    "To avoid any problems with GitHub blocking us from downloading the data many times, we have downloaded and saved a snapshot of the html files for you in the [data](data) folder. Note that the data folder is not completely consistent with what you see on the web â€“ we've made a few patches to the data that makes your task here easier and this data represents a snapshot in time. You will be treating the data folder as your website to be scraped. The path to data folder is stored in `DATA_PATH` variable.\n",
    "\n",
    "In the data folder you will find first 10 pages of highly starred repositories saved as `searchPage1.html`,`searchPage2.html`,`searchPage3.html` ... `searchPage10.html`\n",
    "\n",
    "Check out page 10 if you want to see what happens if you scrape too quickly ðŸ˜‰. \n",
    "\n",
    "Now read these html files in python and create a soup object. This is a two step process:\n",
    " * Read the text in the html files\n",
    " * Create the soup from the files that you've read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read html files and create soup files\n",
    "html_pages = []\n",
    "for files in os.listdir(DATA_PATH):\n",
    "    if files.endswith(\".html\"):\n",
    "        full_path = os.path.join(DATA_PATH, files)\n",
    "        with open(full_path, 'r') as f:\n",
    "            html_pages.append(f.read())\n",
    "\n",
    "SearchPage_soup = []\n",
    "for page in html_pages:\n",
    "    SearchPage_soup.append(BeautifulSoup(page, 'html.parser'))\n",
    "\n",
    "len(SearchPage_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "# Extract all 'div' elements with class \"mt-n1\" (these classes contain individual repositories) from each item in SearchPage_soup.\n",
    "repositories = []\n",
    "\n",
    "for i in range(0, len(SearchPage_soup)):\n",
    "    mtn1 = SearchPage_soup[i].find_all('div', class_=\"mt-n1\")\n",
    "    repositories.extend(mtn1)\n",
    "\n",
    "print(len(repositories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "['netdata/netdata', 'tonsky/FiraCode', 'denoland/deno', 'h5bp/html5-boilerplate', 'ElemeFE/element', 'adam-p/markdown-here', 'h5bp/Front-end-Developer-Interview-Questions', 'resume/resume.github.com', 'josephmisiti/awesome-machine-learning', 'lodash/lodash', 'angular/angular.js', 'puppeteer/puppeteer', 'mrdoob/three.js', 'microsoft/TypeScript', 'angular/angular', 'microsoft/terminal', 'laravel/laravel', 'moby/moby', 'ant-design/ant-design', 'iluwatar/java-design-patterns', 'ossu/computer-science', '30-seconds/30-seconds-of-code', 'mui-org/material-ui', 'jquery/jquery', 'webpack/webpack', 'reduxjs/redux', 'nvbn/thefuck', 'vuejs/awesome-vue', 'avelino/awesome-go', 'atom/atom', 'apple/swift', 'hakimel/reveal.js', 'MisterBooo/LeetCodeAnimation', 'PanJiaChen/vue-element-admin', 'pallets/flask', 'socketio/socket.io', 'expressjs/express', 'Semantic-Org/Semantic-UI', 'shadowsocks/shadowsocks-windows', 'chartjs/Chart.js', 'jwasham/coding-interview-university', 'kamranahmedse/developer-roadmap', 'github/gitignore', 'airbnb/javascript', 'microsoft/vscode', 'CyC2018/CS-Notes', 'd3/d3', 'flutter/flutter', 'torvalds/linux', 'facebook/react-native', 'nodejs/node', 'TheAlgorithms/Python', 'daneden/animate.css', 'kubernetes/kubernetes', 'justjavac/free-programming-books-zh_CN', 'FortAwesome/Font-Awesome', 'trekhleb/javascript-algorithms', 'tensorflow/models', 'ytdl-org/youtube-dl', 'danistefanovic/build-your-own-x', 'freeCodeCamp/freeCodeCamp', '996icu/996.ICU', 'vuejs/vue', 'facebook/react', 'tensorflow/tensorflow', 'twbs/bootstrap', 'EbookFoundation/free-programming-books', 'sindresorhus/awesome', 'getify/You-Dont-Know-JS', 'ohmyzsh/ohmyzsh', 'donnemartin/system-design-primer', 'electron/electron', 'vinta/awesome-python', 'jackfrued/Python-100-Days', 'facebook/create-react-app', 'public-apis/public-apis', 'axios/axios', 'golang/go', 'Snailclimb/JavaGuide', 'jlevy/the-art-of-command-line', 'django/django', 'elastic/elasticsearch', 'keras-team/keras', 'jakubroztocil/httpie', 'storybookjs/storybook', 'typicode/json-server', 'chrislgarry/Apollo-11', 'spring-projects/spring-boot', 'rails/rails', 'zeit/next.js']\n"
     ]
    }
   ],
   "source": [
    "# Repository Names\n",
    "repo_names = []\n",
    "\n",
    "for repo in repositories:\n",
    "    element = repo.find(\"a\", class_=\"v-align-middle\")\n",
    "    repo_names.append(element.get_text().strip())\n",
    "\n",
    "print(len(repo_names))\n",
    "print(repo_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "['C', 'Clojure', 'TypeScript', 'JavaScript', 'Vue', 'JavaScript', 'HTML', 'JavaScript', 'Python', 'JavaScript', 'JavaScript', 'JavaScript', 'JavaScript', 'TypeScript', 'TypeScript', 'C++', 'PHP', 'Go', 'TypeScript', 'Java', 'None', 'JavaScript', 'JavaScript', 'JavaScript', 'JavaScript', 'TypeScript', 'Python', 'None', 'Go', 'JavaScript', 'C++', 'JavaScript', 'Java', 'Vue', 'Python', 'JavaScript', 'JavaScript', 'JavaScript', 'C#', 'JavaScript', 'None', 'None', 'None', 'JavaScript', 'TypeScript', 'Java', 'JavaScript', 'Dart', 'C', 'JavaScript', 'JavaScript', 'Python', 'CSS', 'Go', 'None', 'JavaScript', 'JavaScript', 'Python', 'Python', 'None', 'JavaScript', 'Rust', 'JavaScript', 'JavaScript', 'C++', 'JavaScript', 'None', 'None', 'None', 'Shell', 'Python', 'C++', 'Python', 'Jupyter Notebook', 'JavaScript', 'Python', 'JavaScript', 'Go', 'Java', 'None', 'Python', 'Java', 'Python', 'Python', 'TypeScript', 'JavaScript', 'Assembly', 'Java', 'Ruby', 'JavaScript']\n"
     ]
    }
   ],
   "source": [
    "# Language\n",
    "langs = []\n",
    "\n",
    "for repo in repositories:\n",
    "    element = repo.find(\"span\", itemprop=\"programmingLanguage\")\n",
    "    if element is None:\n",
    "        langs.append(\"None\")\n",
    "        \n",
    "    else: \n",
    "        langs.append(element.get_text().strip())\n",
    "\n",
    "print(len(langs))\n",
    "print(langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "['44.8k', '44.4k', '44.4k', '44k', '43.8k', '43.8k', '43.6k', '43.6k', '43.5k', '43.5k', '59.7k', '58.6k', '58.4k', '58k', '57.7k', '57.6k', '57.6k', '56.4k', '56.3k', '55.4k', '54.9k', '54.6k', '54.5k', '53k', '52.9k', '52.2k', '52.1k', '52k', '51.9k', '51.1k', '50.6k', '50.3k', '49.7k', '49.2k', '48.9k', '48.8k', '47.3k', '47.3k', '47.3k', '47.2k', '97.8k', '95.7k', '95.2k', '92.6k', '91.5k', '90.8k', '89.9k', '86.7k', '86.5k', '84.9k', '67.7k', '67k', '64.8k', '63.2k', '63.2k', '62.1k', '61.9k', '61.6k', '61.6k', '60.2k', '309k', '249k', '157k', '144k', '141k', '139k', '137k', '126k', '117k', '103k', '82.7k', '80.7k', '79.3k', '78.1k', '76k', '70.6k', '69.6k', '69k', '68.8k', '68.2k', '47.2k', '47k', '46.8k', '45.8k', '45.5k', '45.5k', '45.4k', '45.4k', '45k', '45k']\n"
     ]
    }
   ],
   "source": [
    "# Number of watchers\n",
    "watches = []\n",
    "\n",
    "# Iterate through each repository object\n",
    "for repo in repositories:\n",
    "    # Find the first 'a' element with class 'muted-link' within the current repository object\n",
    "    element = repo.find(\"a\", class_=\"muted-link\")\n",
    "    if element:\n",
    "        text = element.get_text().strip()\n",
    "        # Check if the text is not None and not empty\n",
    "        if text:\n",
    "            watches.append(text)\n",
    "\n",
    "print(len(watches))\n",
    "print(watches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of stars\n",
    "stars = []\n",
    "\n",
    "for repo in repositories:\n",
    "    element = repo.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data\n",
    "\n",
    "Extract the following data for each repository, and create a Pandas Dataframe with a row for each repository and a column for each of these datums. \n",
    "\n",
    "+ The name of the repository\n",
    "+ The primary language (there are multiple or none, if multiple, use the first one, if none, use \"none\")\n",
    "+ The number of watches\n",
    "+ The number of stars\n",
    "+ The number of forks\n",
    "+ The number of issues\n",
    "+ Number of commits\n",
    "+ Number of contributors\n",
    "+ Number of pull requests, and\n",
    "+ Number of top level folders in the file list.\n",
    "\n",
    "Here's an example for one repository, `jackfrued/Python-100-Days,` in our dataset: \n",
    "```python\n",
    "{'name': 'Python-100-Days',\n",
    "'language': 'Jupyter Notebook',\n",
    "'watches': '4822',\n",
    "'stars': '78068',\n",
    "'forks': '30979',\n",
    "'issues': 224,\n",
    "'commits': 296,\n",
    "'contributors': 12,\n",
    "'pull_requests':85,\n",
    "'folders': 14\n",
    "}\n",
    "```\n",
    "\n",
    "### Task 1.3 Extract repository URLs\n",
    "\n",
    "If you look at the results of the 100 most-starred repositories [(this list)](https://github.com/search?o=desc&q=stars%3A%3E1&s=stars&type=Repositories), you will notice that all the information we want to extract for each repository is not in that list. This information is in the repositoryâ€™s individual web page, for example [996icu](https://github.com/996icu/996.ICU). \n",
    "\n",
    "Therefore, you will first have to extract links of each repository from the soup you scraped earlier. When you extract the link for the repository, it will be a path to the stored HTML page for the repository. You will use this path to read the file and extract the above information.\n",
    "\n",
    "Refer to the scraping lecture for details on how to do this. We recommend you use the web inspector to identify the relevant structures.\n",
    "\n",
    "Example of a link that you need to extract - 996icu/996.ICU.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "htmls = []\n",
    "for root, dirs, files in os.walk(DATA_PATH):\n",
    "    for file in files:\n",
    "        if file.endswith(\".html\") and not any(file == f\"searchPage{i}.html\" for i in range(1, 11)):\n",
    "            full_path = os.path.join(root, file)\n",
    "            with open(full_path, 'r') as f:\n",
    "                htmls.append(f.read())\n",
    "\n",
    "soups = []\n",
    "for page in htmls:\n",
    "    soups.append(BeautifulSoup(page, 'html.parser'))\n",
    "\n",
    "print(len(soups))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "['C', 'None', 'None', 'Go', 'None', 'Jupyter Notebook', 'Python', 'TypeScript', 'Python', 'Go', 'Python', 'TypeScript', 'JavaScript', 'None', 'Python', 'C', 'Python', 'Python', 'Vue', 'JavaScript', 'None', 'None', 'Java', 'JavaScript', 'TypeScript', 'Go', 'JavaScript', 'JavaScript', 'Ruby', 'JavaScript', 'Rust', 'None', 'JavaScript', 'HTML', 'JavaScript', 'Assembly', 'C++', 'C#', 'JavaScript', 'JavaScript', 'JavaScript', 'JavaScript', 'JavaScript', 'JavaScript', 'Clojure', 'JavaScript', 'JavaScript', 'None', 'Java', 'Python', 'Dart', 'TypeScript', 'None', 'JavaScript', 'JavaScript', 'JavaScript', 'JavaScript', 'C++', 'Go', 'Python', 'C++', 'JavaScript', 'Java', 'None', 'Java', 'TypeScript', 'Shell', 'Python', 'Java', 'JavaScript', 'Python', 'JavaScript', 'JavaScript', 'JavaScript', 'CSS', 'PHP', 'Vue', 'TypeScript', 'TypeScript', 'C++', 'Java', 'JavaScript', 'None', 'JavaScript', 'Python', 'JavaScript', 'JavaScript', 'JavaScript', 'JavaScript', 'Python']\n"
     ]
    }
   ],
   "source": [
    "# Repository Names\n",
    "repo_names = []\n",
    "\n",
    "for soup in soups:\n",
    "    element = soup.find(\"a\", attrs={\"data-pjax\": \"#js-repo-pjax-container\"})\n",
    "    repo_names.append(element.text)\n",
    "\n",
    "# Languages\n",
    "lang = []\n",
    "\n",
    "for soup in soups:\n",
    "    element = soup.find(\"span\", class_=\"language-color\")\n",
    "    if element:\n",
    "        lang.append(element.text)\n",
    "    else:\n",
    "        lang.append(\"None\")\n",
    "\n",
    "print(len(lang))\n",
    "print(lang)\n",
    "\n",
    "# Number of watchers\n",
    "watchers = []\n",
    "\n",
    "for soup in soups:\n",
    "    element = soup.find(\"a\", class_=\"social-count\", href=lambda x: x and \"/watchers\" in x)\n",
    "    watchers.append(element.text.strip())\n",
    "\n",
    "# Number of stars\n",
    "stars = []\n",
    "\n",
    "for soup in soups:\n",
    "    element = soup.find('a', class_=\"social-count js-social-count\", href=lambda x: x and \"/stargazers\" in x)\n",
    "    stars.append(element.text.strip())\n",
    "\n",
    "# Number of forks\n",
    "forks = []\n",
    "\n",
    "for soup in soups:\n",
    "    soup.find('a', class_=\"social-count\", href=lambda x: x and \"/members\" in x)\n",
    "    forks.append(element.text.strip())\n",
    "\n",
    "# Number of Issues\n",
    "issues = []\n",
    "\n",
    "for soup in soups:\n",
    "    issue_link = soup.find('a', href=lambda x: x and '/issues' in x)\n",
    "    if issue_link:\n",
    "        issue_count = issue_link.find('span', class_='Counter')\n",
    "        if issue_count:\n",
    "            issues.append(issue_count.text.strip())\n",
    "        else:\n",
    "            issues.append(0)\n",
    "    else:\n",
    "        issues.append(0)\n",
    "\n",
    "# Number of commits\n",
    "commits = []\n",
    "\n",
    "for soup in soups:\n",
    "    commit_link = soup.find('a', href=lambda x: x and '/commits/master' in x)\n",
    "    if commit_link:\n",
    "        commit_count = commit_link.find('span', class_='num text-emphasized')\n",
    "        if commit_count:\n",
    "            commits.append(commit_count.text.strip())\n",
    "        else:\n",
    "            commits.append(0)\n",
    "    else:\n",
    "        commits.append(0)   \n",
    "\n",
    "#  Number of contributors\n",
    "contributors = []\n",
    "\n",
    "for soup in soups:\n",
    "    contributors_link = soup.find('a', href=lambda x: x and '/contributors' in x)\n",
    "    if contributors_link:\n",
    "        contributors_count = contributors_link.find('span', class_='num text-emphasized')\n",
    "        if contributors_count:\n",
    "            contributors.append(contributors_count.text.strip())\n",
    "        else:\n",
    "            contributors.append(0)\n",
    "    else:\n",
    "        contributors.append(0)\n",
    "\n",
    "# Number of pulls\n",
    "pulls = []\n",
    "\n",
    "for soup in soups:\n",
    "    pulls_link = soup.find('a', href=lambda x: x and '/pulls' in x)\n",
    "    if pulls_link:\n",
    "        pulls_count = pulls_link.find('span', class_='Counter')\n",
    "        if pulls_count:\n",
    "            pulls.append(pulls_count.text.strip())\n",
    "        else:\n",
    "            pulls.append(0)\n",
    "    else:\n",
    "        pulls.append(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['37', '1', '89', '19', '16', '85', '17', '51', '565', '188', '33', '535', '75', '22', '0', '322', '62', '43', '294', '17', '23', '116', '1', '9', '10', '176', '20', '30', '239', '6', '25', '220', '152', '0', '1', '4', '428', '11', '3', '60', '22', '119', '79', '126', '2', '106', '13', '0', '5', '3', '119', '32', '38', '128', '72', '94', '17', '64', '900', '219', '245', '89', '27', '14', '9', '28', '562', '237', '1', '147', '24', '262', '16', '35', '11', '0', '30', '145', '257', '37', '309', '1', '0', '30', '82', '78', '108', '84', '98', '59']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#'folders': 14\"\"\"\n",
    "pulls = []\n",
    "\n",
    "for soup in soups:\n",
    "    pulls_link = soup.find('a', href=lambda x: x and '/pulls' in x)\n",
    "    if pulls_link:\n",
    "        pulls_count = pulls_link.find('span', class_='Counter')\n",
    "        if pulls_count:\n",
    "            pulls.append(pulls_count.text.strip())\n",
    "        else:\n",
    "            pulls.append(0)\n",
    "    else:\n",
    "        pulls.append(0)\n",
    "\n",
    "print(pulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4 Extracting required information\n",
    "\n",
    "Once you have extracted links for each repository, you can start parsing those HTML pages using BeautifulSoup and extract all the required information.\n",
    "\n",
    "**Note**: There are few repositories which do not contain 'issues' field (such as 996icu/996.ICU.html). Therefore, write your code such that it handles this condition as well.\n",
    "\n",
    "**Save the dataframe you created to a new file project_info.csv and include this in your submission.** This separate file will also be graded and is required to earn points.\n",
    "\n",
    "You also need to make sure that you reformat all numerical columns to be integer data. You can do that either as you parse, or when you have a dataframe with strings.\n",
    "\n",
    "Note that there is one repository flagged as having infinite contributers (the Linux kernel). We'll assume that it in fact has 15600 contributors (an estimate based on a Google search at the time of download)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_info.to_csv('project_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analyzing the repository data\n",
    "\n",
    "In this part, you will analyze the data collected in Part 1 using regression tools. The goal is to identify properties that make a repository popular. \n",
    "\n",
    "First, load the `project_info.csv` file in again. **We need you to do this so that we can run your code below without having to run your scraping code, which can be slow.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_info = pd.read_csv('project_info.csv')\n",
    "project_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 Describe the data\n",
    "\n",
    "+ Get an overview of the data using the describe function.\n",
    "+ Compute the correlation matrix, visualize it with a heat map.\n",
    "+ Visualize the correlations by making a scatterplot matrix.\n",
    "+ Interprete what you see.\n",
    "\n",
    "You can re-use code from your previous homework here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Linear regression\n",
    "\n",
    "1. Use linear regression to try to predict the number of Stars based on Forks, Pull Requests, and Number of Folders. Explain why this is not a very good model by discussing the R-squared , F-statistic p-value, and coefficient  p-values. \n",
    "+ Develop another model which is better. Explain why it is better and interpret your results. Hint: try using other variables such as Watches and/or Contributors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your interpretation:** TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
