{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science â€“ Homework 7 - March 22, 2024 \n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "Due: Friday, March 22, 11:59pm.\n",
    "\n",
    "In this homework, you will use classification methods to classify handwritten digits (Part 1) and predict the popularity of online news (Part 2). We hope these exercises will give you an idea of the broad usage of classification methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Data\n",
    "First Name:\n",
    "<br>\n",
    "Last Name:\n",
    "<br>\n",
    "E-mail:\n",
    "<br>\n",
    "UID:\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports and setup \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import tree, svm, metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1: MNIST handwritten digits\n",
    "\n",
    "The MNIST handwritten digit dataset consists of images of handwritten digits, together with labels indicating which digit is in each image. You will see that images are just matrices with scalar values, and that we can apply the classification algorithms we studied on them.\n",
    "\n",
    "In this problem, we'll use scikit-learn to compare classification methods on the MNIST dataset. \n",
    "\n",
    "There are several versions of the MNIST dataset. We'll use the one that is built-into scikit-learn, described [here](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html). \n",
    "\n",
    "1) Classes: 10 (one for each digit)\n",
    "2) Samples total: 1797\n",
    "3) Samples per class: $\\approx$180\n",
    "4) Dimensionality: 64 (8 pixels by 8 pixels)\n",
    "5) Features: integers 0-16 (grayscale value; 0 is white, 16 is black)\n",
    "\n",
    "Here are some examples of the images. Note that the digits have been size-normalized and centered in a fixed-size ($8\\times8$ pixels) image.\n",
    "\n",
    "<img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_digits_classification_001.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we will scale the data before running them through our algorithms. You can read details about scaling and why it's important [here](http://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "#X = digits.data\n",
    "X = scale(digits.data)\n",
    "y = digits.target\n",
    "print(type(X))\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "n_digits = len(np.unique(digits.target))\n",
    "print(\"n_digits: %d, n_samples %d, n_features %d\" % (n_digits, n_samples, n_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is what one digit (a zero) looks like\n",
    "\n",
    "print(\"===\\nThe digit\")\n",
    "print(digits.target[0])\n",
    "\n",
    "print(\"===\\nThe raw data\")\n",
    "print(digits.images[0])\n",
    "\n",
    "print(\"===\\nThe scaled data\")\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot a few of the images\n",
    "\n",
    "plt.figure(figsize= (10, 10))    \n",
    "for ii in np.arange(25):\n",
    "    plt.subplot(5, 5, ii+1)\n",
    "    plt.imshow(np.reshape(digits.images[ii,:],(8,8)), cmap='Greys',interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might find [this webpage](http://scikit-learn.org/stable/tutorial/basic/tutorial.html) to be generally helpful for this exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Classification with Support Vector Machines (SVM)\n",
    "\n",
    "1. Split the data into a training and test set using the command \n",
    "```\n",
    "train_test_split(X, y, random_state=1, test_size=0.8)\n",
    "```\n",
    "    1. Use SVM with an `rbf` kernel and parameter `C=100` to build a classifier using the *training dataset*.\n",
    "    2. Using the *test dataset*, evaluate the accuracy of the model. Again using the *test dataset*, compute the confusion matrix. What is the most common mistake that the classifier makes? **Note: this corresponds to the largest off-diagonal entry of the confusion matrix.**\n",
    "    3. Print all of these misclassified digits as images. \n",
    "    4. Using the 'cross_val_score' function, evaluate the accuracy of the SVM for 100 different values of the parameter C between 1 and 100. What is the best value? \n",
    "    5. Try to train and test the algorithm on the raw (non-scaled) data. Report the accuracy score and confusion matrix for different settings of `gamma`. You may use whatever C value you determined worked well on the scaled data in \"part 4\" .What is your observation of the results on the raw data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Prediction with k-nearest neighbors\n",
    "`Repeat` task 1.1 using k-nearest neighbors (k-NN). In part 2, use k=10. In part 5, find the best value of k. \n",
    "Work on every single question except the last one (You don't need to handle the non-scaled data here) in task 1.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Popularity of online news\n",
    "\n",
    "For this problem, you will use classification tools to predict the popularity of online news based on attributes such as the length of the article, the number of images, the day of the week that the article was published, and some variables related to the content of the article. You can learn details about the datasetat the\n",
    "[UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity). \n",
    "This dataset was first used in the following conference paper: \n",
    "\n",
    "K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. *Proceedings of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence* (2015).\n",
    "\n",
    "The dataset contains variables describing 39,644 articles published between January 7, 2013 and Januyary 7, 2015 on the news website, [Mashable](http://mashable.com/). \n",
    "There are 61 variables associated with each article. Of these, 58 are *predictor* variables, 2 are variables that we will not use (url and timedelta), and finally the number of shares of each article. The number of shares is what we will use to define whether or not the article was *popular*, which is what we will try to predict. You should read about the predictor variables in the file *OnlineNewsPopularity.names*. Further details about the collection and processing of the articles can be found in the conference paper. \n",
    "\n",
    "\n",
    "### Task 2.1 Import the data \n",
    "* Use the pandas.read_csv() function to import the dataset. Then, **print** shape of the data and first few lines of data.\n",
    "* To use [scikit-learn](http://scikit-learn.org), we'll need to save the data as a numpy array. Use the *DataFrame.values* command to export the predictor variables as a numpy array called *X* this array should not include our target variable (the number of shares). We don't need the url and timedelta, so let's drop these columns. \n",
    "* Export the number of shares as a separate numpy array, called *shares*. We'll define an article to be popular if it received more shares than the median number of shares. Create a binary numpy array, *y*, which indicates whether or not each article is popular.\n",
    "* Print *y* array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Exploratory data analysis \n",
    "\n",
    "First check to see if the values are reasonable. What are the min, median, and maximum number of shares? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 Classification using k-NN\n",
    "\n",
    "Develop a k-NN classification model for the data. Use cross validation to choose the best value of k. What is the best accuracy you can obtain on the test data? \n",
    "Plot graph of accuracy with various values of k to show your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4 Classification using SVM\n",
    "\n",
    "Develop a support vector machine classification model for the data.Show the results of cross-validation along with best parameter at the end.\n",
    "\n",
    "\n",
    "*Hint:* SVM is more computationally expensive, so you might want to start by using only a fraction of the data, say 4,500 articles. It takes multiple minutes to run on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5 Classification using decision trees\n",
    "\n",
    "Develop a decision tree classification model for the data. Use cross validation to choose good values of the max tree depth (*max_depth*) and minimum samples split (*min_samples_split*). \n",
    "Show the results of cross-validation along with best parameter at the end.\n",
    "We don't need a plot here, we only need cross validation output and the optimal setting of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6 Describe your findings\n",
    "1. Which method (k-NN, SVM, Decision Tree) worked best?\n",
    "    1. How did different parameters influence the accuracy?\n",
    "    2.  Which model is easiest do interpret?\n",
    "    3. How would you interpret your results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Solution:** TODO"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
