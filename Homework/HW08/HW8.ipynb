{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyjMtMigNNKu"
   },
   "source": [
    "# Introduction to Data Science â€“ Homework 8\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "Due: Friday, April 12 2024, 11:59pm.\n",
    "\n",
    "In this homework, you will use clustering, principal component analysis, regular expressions, and natural language processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1g-mA9VINNKy"
   },
   "source": [
    "## Your Data\n",
    "First Name:\n",
    "<br>\n",
    "Last Name:\n",
    "<br>\n",
    "E-mail:\n",
    "<br>\n",
    "UID:\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxCk-ZsZNNKz"
   },
   "outputs": [],
   "source": [
    "# imports and setup \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "\n",
    "from sklearn import tree, svm, metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7PcNdb2NNK0"
   },
   "source": [
    "## Part 1: Analyze US Fast Food data\n",
    "\n",
    "We'll analyze a dataset describing prevalence of some fast food restaurants by US State. Theses are subsetted/cleaned from the [Kaggle dataset by Rishi Darmala](https://www.kaggle.com/datasets/rishidamarla/fast-food-restaurants-in-america) and presented as number of each restaurant per million residents. \n",
    "\n",
    "Our goal will be to use unsupervised methods to understand how prevalence of these restaurants differ between states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UZS1w4vNNK1"
   },
   "source": [
    "### Task 1.1 Import the data and perform some preliminary exploratory analysis. \n",
    "Use the *read_csv* pandas function to import the data as a dataframe. (Note - Index of this dataframe should be the names of the state)\n",
    "\n",
    "Plot a scatterplot matrix of the data. \n",
    "- Explore basic statistics of the data. \n",
    "- Describe how the variables are correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "L-J-WAj-NNK1",
    "outputId": "9d28f103-cbd3-4c07-a7eb-29a398d89892",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6Eg84UsNNK3"
   },
   "source": [
    "**Your description:** TODO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRS-UlH1NNK4"
   },
   "source": [
    "### Task 1.2 - Cluster Heat Map\n",
    "\n",
    "Generate a [cluster heat map](https://seaborn.pydata.org/generated/seaborn.clustermap.html) with a dendrogram using seaborn (see lecture). Be sure to standardize the dataset using the `standard_scale=1` parameter.\n",
    "\n",
    "- How would you interpret this cluster map? \n",
    "- Describe any patterns you see.\n",
    "- Is there anything in this data that surprises you? Do you trust it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XFROcBz2NNK4",
    "outputId": "13b7efd4-2dc3-474b-d9a5-29c85d79e1c7"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7_J0qSaNNK5"
   },
   "source": [
    "**Your Interpretation**: TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilyAU6QHNNK6"
   },
   "source": [
    "### Task 1.3 Visualize the data using PCA\n",
    "\n",
    "Complete the following steps:\n",
    "1. Scale the dataset using the *scale* function of the sklearn.preprocessing library. \n",
    "+ Calculate the principal components of the dataset. \n",
    "+  Store the principal components in a pandas dataframe. (Note - Index of this dataframe should be the names of the state) \n",
    "+ Plot a scatterplot of PC1 and PC2. Using the matplotlib function *annotate*, use the state names as markers (instead of dots). From this scatterplot, can you tell approximately how many clusters our dataset shall have?\n",
    "+ Print the explained variance ratio of the PCA. Plot the explained variance ratio of the PCA. After observing the explained variance ratio, how many dimensions would you reduce your data to? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buOfLGlRNNK7"
   },
   "source": [
    "**Your description:** TODO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U08wV5YHNNK-"
   },
   "source": [
    "### Task 1.4 k-means cluster analysis\n",
    "\n",
    "1. Using k-means, cluster the states into two clusters. **Use the scaled dataset**. Which states belong to which clusters?\n",
    "2. Vary k (between 2 and 20) and check if there could be a better value for k. If yes, what is that value? Also, describe how did you find that value?\n",
    "3. Use the principal components to plot the clustering corresponding to the k you chose in the previous question. Again label each point using the state name and this time color the states according to the clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6A-xM4j6NNK-",
    "outputId": "575c83ab-a139-47b5-9f67-ae448f3199ad"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhjGBKd8NNK_"
   },
   "source": [
    "**Interpretation for best K:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnBhLF8INNLB"
   },
   "source": [
    "**Interpretation for PCA and K-Means**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Db9sKspvNNLC"
   },
   "source": [
    "### Task 1.5 Hierarchical cluster analysis\n",
    "\n",
    "1.  Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states into the number of clusters you determined for k-means. Which states belong to which clusters? \n",
    "2. Visualize your cluster results on top of the first two principle components, as before.\n",
    "3. Do you get similar results as for k-means? Can you see trends between the states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5_iixj0TNNLD",
    "outputId": "786e0d0f-acff-4312-f258-8bba6b07a27e"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTIg2IRLNNLE"
   },
   "source": [
    "**Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mk483UanNNLE"
   },
   "source": [
    "### Task 1.6 DBSCAN\n",
    "\n",
    "1.  Using DBSCAN and experiment with different values for $\\epsilon$ and min samples. Which states belong to which clusters? \n",
    "2. Visualize your cluster results on top of the first two principle components, as before.\n",
    "3. Do you get similar results as before? Is DBSCAN stable or very sensitive to changes in epsilon for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "G9vcfaxiNNLF",
    "outputId": "d908db80-1faf-400c-c1bd-041fd0875fc0"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smDYeGVrNNLF"
   },
   "source": [
    "**Your Interpretation**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "ToFFbDKMNNLG"
   },
   "source": [
    "# 2. Regular Expressions \n",
    "\n",
    "Write regular expressions for the following examples that matches the data of the given format and any other reasonable variations thereof. E.g., your regex shouldn't be specific to one URL or one phone number, but should work for all examples of the same format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Egc1n8B6NNLG"
   },
   "source": [
    "**Task 2.1.** Writes a regular expression that extracts the urls out of this string, specifically the part after the `https://`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RskLUcFINNLG",
    "outputId": "64dc701c-033c-423e-9af5-13431914191e"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"To find out more about the math department, go to: https://www.math.utah.edu/\n",
    "Alternatively, to find out more about computing, go to: https://www.cs.utah.edu/ \n",
    "You may also find out about data science at: https://datasciencecourse.net/2024/\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvKt2Ft_NNLH",
    "outputId": "810aa499-5406-4ac9-fdc7-24fda0d2be2f"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFBdBFRjNNLI"
   },
   "source": [
    "**Task 2.2.** Write a regular expression that extracts all phone numbers and fax numbers from this text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2R6gVytNNLI"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"You can reach me at 415-273-9164, or my office at (212) 555-2368 or (212) 606-0842.\\ \n",
    "Send me a fax at 857 555 0164. We finally made the sale for all 196 giraffes.\\\n",
    "They purchase order is 376 152.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kM7hddDUNNLJ",
    "outputId": "a176bd40-bb85-42c6-c83a-0a008fa40c2c"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aFhecJHNNLJ"
   },
   "source": [
    "**Task 2.3.** Write a regular expression that extracts the entirety of all closing html tags (e.g., including the `<` and `>`) from `html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXISkKwYNNLJ"
   },
   "outputs": [],
   "source": [
    "html = \"\"\"<center>This is <b>important</b> and <u>very</u><i>timely</i><br />.\\\n",
    "</center> Was this <span> what you meant?</span>.<video>intro</video>.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLAe-YhmNNLK",
    "outputId": "37833adf-8f1a-4b6a-982e-80df4aa53bf5"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmvHXkk3NNLK"
   },
   "source": [
    "**Task 2.4.** Write a regular expression that extracts all the names of people, real or fictitious, from the following text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KO93wHQFNNLL"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Set in Hollywood, the comedy drama stars Will Arnett, Amy Sedaris, Alison Brie, \\\n",
    "Paul F. Tompkins, and  Aaron Paul.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZIqKoMhNNLL",
    "outputId": "74e3d454-52ed-4a2b-dda3-e9a7a040bd5f"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-JGz3z6NNLM"
   },
   "source": [
    "**Task 2.5.** Write a regular expression that extracts all the image URLs from the html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4lIwsloNNLM"
   },
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "<img class=\"dataimage\" src=\"https://datasciencecourse.net/2024/assets/i/teaser.png\">\n",
    "<img width=\"140\" height=\"140\" src=\"https://www.math.utah.edu/_resources/images/web-buttons/MajorsMinors.png\">\n",
    "<img class=\"srcimage\" src=\"https://csszengarden.com/211/title01.jpg\">\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tP_bZ8DXNNLN",
    "outputId": "148bfd8f-69f2-4a11-cfba-08852fd31ae6"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1onnuOjNNLN"
   },
   "source": [
    "## 3. NLP: Classifying Newsgroups Articles\n",
    "\n",
    "Newsgroups were the social media of the 80s and 90s. Newsgroups are open discussion forums structured into hierarchies. For example, the following newsgroups cover topics as diverse as atheism, computer graphics, and classified ads.  \n",
    "\n",
    "```\n",
    "alt.atheism\n",
    "comp.graphics\n",
    "comp.os.ms-windows.misc\n",
    "comp.sys.ibm.pc.hardware\n",
    "comp.sys.mac.hardware\n",
    "comp.windows.x\n",
    "misc.forsale\n",
    "```\n",
    "\n",
    "We will be combining machine learning and natural language processing to classify the news articles into these groups. We expect, for example, that the text for a classified ad in `misc.forsale` is different from text in `alt.atheism`. \n",
    "\n",
    "We will use the 20 Newsgroups corpus from scikit-learn. The 20 newsgroups dataset comprises around 18,000 newsgroups posts on 20 topics. The general steps we follow are:\n",
    "1. Load the corpus    \n",
    "2. Do preprocessing: removal of stopwords, stemming, etc.\n",
    "3. Vectorize the text\n",
    "4. Split into training and test sets\n",
    "5. Train our classifier\n",
    "\n",
    "Refer to documentation on the [20 newsgroups dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) to learn about the dataset and find out how to download it.\n",
    "We recommended you use the `subset='all'` parameter to load all the data at once, instead of `subset='train'` and `subset='test'` separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9I4vTUHuNNLO"
   },
   "source": [
    "### Task 3.1 Load the dataset.\n",
    "\n",
    "1. Print the exact number of news articles in the corpus.\n",
    "2. Print all 20 categories the news articles belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccVn80OPNNLO",
    "outputId": "ed7b35b0-926f-4a2c-ecb8-a50292bb1ff6"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0lp7NEVNNLP"
   },
   "source": [
    "### Task 3.2 Classification\n",
    "\n",
    "Vectorize the data using vectorizers from sklearn. Using these vectors as features and the article category from corpus as labels, train a NaiveBayes classifier to classify the data.\n",
    "\n",
    "#### Vectorizers\n",
    "\n",
    "Vectorizes help us to transform text data into features we can use in machine learning. We did the vectorization manually in class, here you will use pre-build vectorizers. \n",
    "\n",
    "You should use CountVectorizer and TfidfVectorizer vectorizers from sklearn to vectorize your data. Please refer to documentation on both to learn how to use them.\n",
    "+ http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "+ http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "Compare the performance of a Naive Bayes classifier (see details below) using both vectorizers. You are encouraged to experiment with different parameters like max_df, min_df, etc. See docs for the meanings.\n",
    "\n",
    "#### Naive Bayes\n",
    "**Resources**\n",
    "1. https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "2. https://www.geeksforgeeks.org/naive-bayes-classifiers\n",
    "3. http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "We will be using Multinomial Naive Bayes from sklearn. Refer to documentation above for how to import the classifier. Then it can be used like any other classifier by using fit and predict functions provided on it.\n",
    "e.g:\n",
    "\n",
    "```\n",
    "clf = MulitnomialNB(alpha = 1)\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "```\n",
    "\n",
    "Alpha is also known as the smoothing factor and ranges from 0 (no smoothing) to 1 (Laplace Smoothing). You can experiment with different values to see if you get better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "274KqURVNNLQ",
    "outputId": "29479224-9f60-4ab9-cfbc-40334aa6cd6e"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3jzRydzNNLR"
   },
   "source": [
    "### Task 3.3 Removing Stopwords\n",
    "\n",
    "Now we'll remove the stopwords to improve our data vectors. TfidfVectorizer and CountVectorizer both can take an argument called stop_words. The words passed to this argument are considered as stopwords and are not vectorized. Then evaluate the new vectors using Multinomial Naive Bayes.\n",
    "\n",
    "Answer the following questions:\n",
    "1. What accuracy were you able to achieve? \n",
    "2. What was the influence of the different vectorizers and the stopword removal? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5_CDeQzwNNLS",
    "outputId": "6022c017-f275-46a0-9dd4-62e15eb28747"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ypq0WpXnNNLS"
   },
   "source": [
    "**Interpretation:** TODO"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
