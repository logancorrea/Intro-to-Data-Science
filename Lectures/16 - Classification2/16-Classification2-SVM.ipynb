{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Data Science \n",
    "# Lecture 16 : Classification 2 - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm, metrics # Support vector machine\n",
    "from sklearn.datasets import make_moons, load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification \n",
    "\n",
    "Recall that in *classification* we attempt to predict a categorical variable based on several features or attributes. \n",
    "\n",
    "We've already seen two methods for classification: \n",
    "+ k Nearest Neighbors (k-NN)\n",
    "+ Decision Trees\n",
    "\n",
    "Another method for classification is [Support Vector Machines (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine). These methods are especially good at classifying small to medium sized, complex datasets. \n",
    "\n",
    "For more info, see:\n",
    "+ G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning, Ch. 9 [digitial version available here](https://www.statlearning.com/)\n",
    "\n",
    "We'll use the [sk-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "**Idea:** Use tools from optimization to find the *best* lines (or hyperplanes or curves) that divide the two datasets. \n",
    "\n",
    "### Simplest version first: 2 classes and no \"kernel transformation\"\n",
    "\n",
    "**Data:** Given $n$ samples $(\\vec{x}_1,y_1), (\\vec{x}_2,y_2),\\ldots,(\\vec{x}_n,y_n)$, where $\\vec{x}_i$ are attributes or features and $y_i$ are categorical variables that you want to predict. We assume that each $\\vec{x}_i$ is a real vector and the $y_i$ are either $1$ or $-1$ indicating the class. \n",
    "\n",
    "**Goal:**  Find the \"maximum-margin line\" or more generally, the \"maximum-margin hyperplane\" that divides the two classes, which is defined so that the distance between the hyperplane and the nearest point in either group is maximized. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/2a/Svm_max_sep_hyperplane_with_margin.png\" width=\"400\">\n",
    "\n",
    "$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad$ \n",
    "source: [wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "\n",
    "\n",
    "We can write any line as the set of points $\\vec{x}$ satisfying \n",
    "$$\\vec{w}\\cdot\\vec{x} = b$$ \n",
    "for some (normal) vector $\\vec{w}$ and number $b$. The goal is to determine $\\vec{w}$ and $b$. Once we have the best values of $\\vec{w}$, $b$, our classifier is simply\n",
    "$$\n",
    "\\vec{x} \\mapsto \\textrm{sgn}(\\vec{w} \\cdot \\vec{x} - b).\n",
    "$$\n",
    "\n",
    "**Hard-margin.** If the data is linearly separable, then we find the separating line (hyperplane) with the *largest margin* ($1/\\|w\\|$). \n",
    "\n",
    "This is given by the solution to the optimization problem \n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\vec{w},b}  & \\  \\|\\vec{w}\\|^2  \\\\ \n",
    "\\textrm{ subject to } & \\  y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1 , \\textrm{for } i = 1,\\,\\ldots,\\,n \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "This can be written as a *convex optimization problem* and efficiently solved. \n",
    "\n",
    "From the picture, the max-margin hyperplane is determined by the $\\vec{x}_i$ which are closest to it. These $\\vec{x}_i$ are called *support vectors*.\n",
    "\n",
    "\n",
    "**Soft-margin.** For data that is not linearly separable (e.g., two moons dataset), we introduce a *penalty* or *loss* function for violating the constraint,  $y_i(\\vec{w}\\cdot\\vec{x}_i + b) \\ge 1$. One of these is the *hinge loss* function, given by \n",
    "$$\n",
    "g(\\vec{x}_i; \\vec{w},b) = \\max\\left(0, 1-y_i(\\vec{w}\\cdot\\vec{x}_i - b)\\right). \n",
    "$$\n",
    "We can see that this function is zero if the constraint is satisfied. If the constraint is not satisfied, we pay a penatly, which is proportional to the distance to the separating hyperplane.\n",
    "\n",
    "We then fix the parameter $C > 0$ and solve the problem \n",
    "$$\n",
    "\\min_{\\vec{w},b}  \\ \\frac{1}{n} \\sum_{i=1}^n g(\\vec{x}_i; \\vec{w},b)  \\ + \\ C \\|\\vec{w}\\|^2  . \n",
    "$$\n",
    "The parameter $C$ determines how much we penalize points for being on the wrong side of the line. \n",
    "\n",
    "**Question:** How to choose the parameter $C$? Cross Validation! (more on this below)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVM and two moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# there are two features contained in X and the labels are contained in y\n",
    "X,y = make_moons(n_samples=500,random_state=1,noise=0.3)\n",
    "\n",
    "# Plot the data, color by class\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"DarkBlue\", marker=\"s\",label=\"class 1\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"DarkRed\", marker=\"o\",label=\"class 2\")\n",
    "plt.legend()\n",
    "plt.title('Two Moons Dataset')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='linear', C=10)\n",
    "\n",
    "# Note for a 'linear' kernel, there is a faster method:\n",
    "#\n",
    "# from sklearn.svm import LinearSVC\n",
    "# model = LinearSVC(C=10,loss=\"hinge\")\n",
    "\n",
    "model.fit(X, y)    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data, color by class\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"DarkBlue\", marker=\"s\",label=\"class 1\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"DarkRed\", marker=\"o\",label=\"class 2\")\n",
    "plt.legend(scatterpoints=1)\n",
    "\n",
    "# Plot the predictions made by SVM\n",
    "x_min, x_max = X[:,0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))\n",
    "zz = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, zz, cmap=ListedColormap(['DarkRed', 'DarkBlue']), alpha=.2)\n",
    "plt.contour(xx, yy, zz, colors=\"black\", alpha=1, linewidths=0.2) \n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.title('Classification of Two Moons using SVM')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix:')\n",
    "y_pred = model.predict(X)\n",
    "print(metrics.confusion_matrix(y_true = y, y_pred = y_pred))\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y, y_pred = y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this is accuracy on the \"training\" data, since we are evaluating the performance on the same data used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines \n",
    "\n",
    "**Basic idea:** The dataset may be more linearly separable if we apply a *nonlinear kernel transformation*. \n",
    "\n",
    "**Example:** two \"rings\" of points\n",
    "\n",
    "**Algorithmic challenge:** Not increasing the computational complexity of the optimization problem. \n",
    "\n",
    "\n",
    "**(Very) rough sketch:**\n",
    "Replace the linear classifier \n",
    "$$\n",
    "\\vec{x} \\mapsto \\textrm{sgn}(\\vec{w} \\cdot \\vec{x} - b).\n",
    "$$\n",
    "with a nonlinear one \n",
    "$$\n",
    "\\vec{x} \\mapsto \\textrm{sgn}( \\vec{w} \\cdot \\phi(\\vec{x}) - b).\n",
    "$$\n",
    "\n",
    "Idea: the function $\\phi$ defines a nonlinear embedding of the data, and one can then do linear SVM in that new space. Think about the example {(-2, 1), (-1, -1), (1, -1), (2, 1)}.\n",
    "\n",
    "Note: one doesn't actually have to specify $\\phi(\\vec{x})$; one just needs to specify the associated kernel $K(\\vec{x},\\vec{y}) = \\phi(\\vec{x}) \\cdot \\phi(\\vec{y})$. This is called the kernel trick.  \n",
    "Choices of kernel in scikit-learn are:\n",
    "- linear: $K(\\vec{x},\\vec{y})=\\langle \\vec{x}, \\vec{y}\\rangle$.\n",
    "- polynomial: $K(\\vec{x},\\vec{y})=(\\gamma \\langle \\vec{x}, \\vec{y}\\rangle + r)^d$. $d$ is specified by keyword degree, $r$ by coef0.\n",
    "- rbf: $K(\\vec{x},\\vec{y})=\\exp(-\\gamma |\\vec{x}-\\vec{y}|^2)$. $\\gamma$ is specified by keyword gamma, must be greater than 0.\n",
    "- sigmoid:  $K(\\vec{x},\\vec{y})=\\tanh(\\gamma \\langle \\vec{x},\\vec{y}\\rangle + r)$, where $r$ is specified by coef0.\n",
    "\n",
    "The most popular choice is rbf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='rbf',C=20,gamma='scale')\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot the data, color by class\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"DarkBlue\", marker=\"s\",label=\"class 1\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"DarkRed\", marker=\"o\",label=\"class 2\")\n",
    "plt.legend(scatterpoints=1)\n",
    "\n",
    "# Plot the predictions made by SVM\n",
    "x_min, x_max = X[:,0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))\n",
    "zz = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, zz, cmap=ListedColormap(['DarkRed', 'DarkBlue']), alpha=.2) #draw filled contours\n",
    "plt.contour(xx, yy, zz, colors=\"black\", alpha=1, linewidths=0.2) #draw contour lines\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.title('Classification of Two Moons using SVM')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix:')\n",
    "y_pred = model.predict(X)\n",
    "print(metrics.confusion_matrix(y_true = y, y_pred = y_pred))\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y, y_pred = y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More than 2 classes?  \n",
    "There are a few different methods for extending binary classification to more than 2 classes. \n",
    "-  **one vs. one:** Consider all pairs of classes (if there are $k$ classes, then there are $\\binom k 2$ different pairs of classes. For each pair, we develop a classifier. For a new point, we perform all classifications and then choose the class that was most frequently assigned. (The classifiers all vote on a class.)\n",
    "\n",
    "- **all vs. one:**  We compare one of the $k$ classes to the remaining $k-1$ classes. We assign the observation to the class which we are most confident (we have to make this precise).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Dataset: The Iris dataset\n",
    "\n",
    "This dataset contains 4 features (attributes) of 50 samples containing 3 different species of iris plants. The goal is to classify the species of iris plant given the attributes. \n",
    "\n",
    "**Classes:**\n",
    "+ Iris Setosa \n",
    "+ Iris Versicolour \n",
    "+ Iris Virginica\n",
    "\n",
    "**Features (attributes):**\n",
    "+ sepal length (cm) \n",
    "+ sepal width (cm) \n",
    "+ petal length (cm) \n",
    "+ petal width (cm) \n",
    "\n",
    "<img src=\"iris.png\" title=\"http://mirlab.org/jang/books/dcpr/dataSetIris.asp?title=2-2%20Iris%20Dataset\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = sns.load_dataset(\"iris\") # built-in dataset in seaborn \n",
    "sns.pairplot(df, hue=\"species\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# import data, scikit-learn also has this dataset built-in\n",
    "iris = load_iris()\n",
    "\n",
    "# For easy plotting and interpretation, we only use first 2 features here. \n",
    "# We're throwing away useful information - don't do this at home! \n",
    "X = iris.data[:,:2]\n",
    "y = iris.target\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y,  marker=\"o\", cmap=cmap_bold, s=30)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)    \n",
    "plt.title('Iris dataset')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# SVM \n",
    "svm_iris = svm.SVC(kernel='rbf',C=3,gamma='scale')\n",
    "svm_iris.fit(X, y)\n",
    "\n",
    "# plot classification \n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))\n",
    "zz = svm_iris.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "#plt.pcolormesh(xx, yy, zz, cmap=cmap_light)\n",
    "plt.contourf(xx, yy, zz, cmap=cmap_light, alpha=.2)\n",
    "\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=30)\n",
    "\n",
    "plt.title('Classification of Iris dataset using SVM')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix:')\n",
    "y_pred = svm_iris.predict(X)\n",
    "print(metrics.confusion_matrix(y_true = y, y_pred = y_pred))\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y, y_pred = y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross Validation \n",
    "\n",
    "**Recall:** Cross validation is a method for assessing how well a model will generalize to an independent data set. \n",
    "\n",
    "**Idea:** We split the dataset into a *training set* and a *test set*. We train the model on the training set. We measure the accuracy of the model on the test set. \n",
    "\n",
    "There are several different ways to do this. Two popular methods are: \n",
    "- **k-fold cross validation** The data is randomly partitioned into k (approximately) equal sized subsamples (folds). For each of the k folds, the method is trained on the other k-1 folds and tested on that fold. The accuracy is computed using each of the k folds as the test set. \n",
    "- **leave-one-out cross validation** Same as above with k=n\n",
    "\n",
    "![4fold_CV](4fold_CV.png)\n",
    "Image [source](https://www.mathworks.com/discovery/cross-validation.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variety of function are implemented in scikit-learn for cross validation. \n",
    "\n",
    "- The [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function randomly splits the data for cross validation\n",
    "- The [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function splits the data and measures accuracy on the test set.  The parameter *cv* is the k in k-fold cross validation. The parameter *scoring* specifies how you want to [evaluate the model](https://scikit-learn.org/stable/modules/model_evaluation.html) (we can just use accuracy). \n",
    "- The [cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) function evaluate metric(s) by cross-validation and also records fit/score times. So, cross-validate is different from cross_val_score in two ways:\n",
    "    1. It allows specifying multiple metrics for evaluation.\n",
    "    2. It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# note: here we are again using all 4 features of the data\n",
    "scores = cross_val_score(estimator = svm_iris, X = iris.data, y = iris.target, cv=5, scoring='accuracy')\n",
    "print(scores)\n",
    "\n",
    "scoring = ['accuracy','precision_macro','recall_macro']\n",
    "scores = cross_validate(estimator = svm_iris, X = iris.data, y = iris.target, cv=5, scoring=scoring)\n",
    "print(sorted(scores.keys()))\n",
    "print(scores['test_accuracy'])\n",
    "print(scores['test_recall_macro'])\n",
    "print(scores['test_precision_macro'])\n",
    "print(scores['fit_time'])\n",
    "print(scores['score_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Incorporating cross-validation to choose SVM parameter, $C$\n",
    "\n",
    "Recall above that when the data is not linearly separable, we introduce a loss function $g$ which penalizes the violation of datapoints lying on the wrong side of the hyperplane and solve the problem \n",
    "$$\n",
    "\\min_{\\vec{w},b}  \\ \\frac{1}{n} \\sum_{i=1}^n g(x_i; \\vec{w},b)  \\ + \\ C \\|\\vec{w}\\|^2  . \n",
    "$$\n",
    "Here, the parameter $C>0$ determines how much we penalize points for being on the wrong side of the line. \n",
    "\n",
    "Generally, the function *get_params()* can be used to see what parameters are available to change in a model. \n",
    "\n",
    "We can use cross-validation to choose the parameter $C$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(svm_iris.get_params())\n",
    "\n",
    "Cs = np.linspace(.01,200,100)\n",
    "Accuracies = np.zeros(Cs.shape[0])\n",
    "for i,C in enumerate(Cs): \n",
    "    svm_iris = svm.SVC(kernel='rbf', C = C,gamma='scale')\n",
    "    scores = cross_val_score(estimator = svm_iris, X = iris.data, y = iris.target, cv=5, scoring='accuracy')    \n",
    "    Accuracies[i]  = scores.mean()\n",
    "        \n",
    "plt.plot(Cs,Accuracies)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(Cs[0:10],Accuracies[0:10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Based on the cross validation results, I would choose $C\\approx 7$. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
