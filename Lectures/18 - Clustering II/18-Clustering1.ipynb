{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Introduction to Data Science \n",
    "# Lecture 18: Clustering 1\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "In this lecture, we will introduce clustering, an **unsupervised** learning technique. We'll cover the following topics:\n",
    "* supervised vs. unsupervised learning\n",
    "* clustering \n",
    "* the k-means clustering algorithm\n",
    "\n",
    "Recommended Reading: \n",
    "* G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning, Ch. 10.1 and 10.3. [digitial version available here](https://www.statlearning.com/)\n",
    "* J. Grus, Data Science from Scratch, Ch. 19\n",
    "* [scikit-learn documentation on clustering](http://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised vs unsupervised learning\n",
    "\n",
    "In this course, we've already discussed regression and classification, which are examples of [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) tasks. \n",
    "\n",
    "In **supervised learning**, you \"learn\" a function, $y = f(x)$, that maps an input, $x$, to an output or label, $y$, based on example input-output data. \n",
    "\n",
    "In **regression**, a form of supervised learning, $x$ is a feature vector and $y$ is a real-valued number. \n",
    "\n",
    "In **classification**, $x$ is a feature vector and $y$ is a categorical variable, e.g., a class. \n",
    "\n",
    "One difficulty of supervised learning is that labels can be expensive or impossible to obtain! \n",
    "\n",
    "\n",
    "\n",
    "In **[unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning)**, one tries to learn hidden structure from \"unlabeled\" data. Unsupervised methods are often used to identify a previously unknown structure of the dataset. \n",
    "\n",
    "Examples of unsupervised learning methods and tasks include \n",
    "* Clustering\n",
    "* Dimensionality reduction\n",
    "* Density estimation \n",
    "* Anomaly detection\n",
    "\n",
    "They can also be a useful precursor to visualizing data – many visualization approaches require clustering or dimensionality reduction as a first step.\n",
    "\n",
    "\n",
    "In this lecture, we'll focus on clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering\n",
    "\n",
    "[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) is the task of discovering unknown subgroups in data, which we call *clusters*.  In other words, the **goal** is to partition the dataset into clusters where 'similar' items are in the same cluster and 'dissimilar' items are in different clusters. \n",
    "\n",
    "**Examples:**\n",
    "* Social Network Analysis: Clustering can be used to find communities.\n",
    "* Ecology: cluster organisms that share attributes into species, genus, etc...\n",
    "* Handwritten digits where the digits are unknown.\n",
    "* Discover disease subtypes based on clinical or lab data.\n",
    "\n",
    "**Question:** What is the difference between *Clustering* and *Classification*? \n",
    "\n",
    "There are several **challenges** to clustering:\n",
    "* We must make sense of what it means for two items in the dataset to be *similar* to one another. In other words, we need to develop a “distance” or a “similarity” function and there are many different choices: Euclidean distance, Pearson correlation, Manhattan distance, weighted distances, Jaccard coefficient, ... \n",
    "* Since the data is unlabeled, there is no “ground truth” to compare to. It is difficult to evaluate the accuracy of clustering methods. In particular, we can't use cross validation. There are some generic approaches that we can use though, but it's also common to use visualization and human inspection, and validation through other data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of Clustering Methods\n",
    "\n",
    "1. Partitional Algorithms\n",
    " - divide data into a set of bins\n",
    " - number of bins either manually set (e.g., k-means) or automatically determined (e.g., affinity propagation)\n",
    "\n",
    "\n",
    "2. Hierarchical Algorithms\n",
    " - Produce a *dendrogram* or \"similarity tree\", \n",
    " - clusters can be produced by \"cutting\" the dendrogram\n",
    "\n",
    "\n",
    "3. Bi-Clustering\n",
    " - Clusters dimensions & records\n",
    "\n",
    "\n",
    "4. Fuzzy clustering\n",
    " - probabilistic cluster assignment allows occurrence of elements in multiples clusters\n",
    "\n",
    "\n",
    "Many variants of these ideas are implemented in scikit-learn. [Here](http://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods) is an overview. \n",
    "\n",
    "\n",
    "There are several **methods** for clustering. We'll discuss: \n",
    "* k-means clustering (today)\n",
    "* hierarchical clustering \n",
    "* DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The k-means clustering method\n",
    "\n",
    "**Data:**  A collection of points $\\{x_i\\}$, for $i = 1,\\ldots n$, where $x_i\\in \\mathbb R^d$. \n",
    "\n",
    "In [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering), one tries to find $k$ *centers*, $\\{\\mu_\\ell\\}$, $\\ell = 1,\\ldots k$, and assign each point $x$ to a *cluster* $C_\\ell$ with center $\\mu_\\ell$, as to minimize the *total intra-cluster distance* \n",
    "$$\n",
    "\\arg\\min_{C} \\sum_{\\ell=1}^k \\sum_{x_i \\in C_\\ell} \\| x_i - \\mu_\\ell\\|^2. \n",
    "$$\n",
    "Here, $\\mu_\\ell$ is the mean of points in $C_\\ell$. The total intra-cluster distance is the total squared Euclidean distance from each point to the center of its cluster. It's a measure of the variance or internal coherence of the clusters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lloyd's Algorithm\n",
    "\n",
    "In practice, k-Means is implemented using [Lloyd's algorithm](https://en.wikipedia.org/wiki/Lloyd%27s_algorithm). \n",
    "\n",
    "**Input:** a set of points $x_1,\\ldots, x_n$ and an integer $k$ (# clusters)\n",
    "\n",
    "Pick $k$ starting points as centers $\\mu_1, \\ldots, \\mu_k$.\n",
    "\n",
    "**while** not converged:\n",
    "1. Assign each point $x_i$, to the cluster $C_\\ell$ with closest center $\\mu_\\ell$. \n",
    "  - For every  point $x_i$ and every center $\\mu_\\ell$, we calculate the distance $d(x_i, \\mu_\\ell)$. \n",
    "  - Then we assign the point $x_i$ to the cluster $C_\\ell$ with the smallest distance.\n",
    "  \n",
    "  \n",
    "2. For each cluster $C_\\ell$ we compute a new center $\\mu_\\ell$ by taking the mean of all $x_i$ assigned to cluster $C_\\ell$, i.e., \n",
    "$$\n",
    "\\mu_\\ell = \\frac{1}{|C_\\ell|}\\sum_{x_i \\in C_\\ell} x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lloyd's Algorithm Illustrated\n",
    "\n",
    "\n",
    "![Lloyd's Algorithm Illustrated](lloyd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some Considerations\n",
    "\n",
    "**Finding K**  \n",
    "How to choose the parameter $k$? Ideas:\n",
    " - visual comparison\n",
    " - compare the value of the total intra-cluster distance for different values of $k$ \n",
    " - try out many $k$'s (optimize some cluster validity criteria)\n",
    " \n",
    "**Initialization**  \n",
    "How do we pick $k$ starting points as centers $\\mu_1, \\ldots \\mu_k$? Ideas:\n",
    " - randomly\n",
    " - points that are far apart? \n",
    " - manually? \n",
    " \n",
    "**Convergence**   \n",
    "What does *converged* mean? Ideas:\n",
    " - no point has changed cluster\n",
    " - distance between old and new centroid below threshold\n",
    " - number of max iterations reached\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## k-means method demo\n",
    "There is a nice visualization of the k-means method by Naftali Harris here:\n",
    "\n",
    "[visualizing-k-means-clustering](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) \n",
    "        \n",
    "![k-means clustering](k-means-fig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of k-means\n",
    "\n",
    "The **computational complexity** is $O(n*k*d*i)$ where \n",
    " - n is the number of data points,\n",
    " - k is the number of clusters,\n",
    " - d is the number of dimensions of the feature vectors,\n",
    " - i is the number of iterations needed until convergence. \n",
    " \n",
    "For data that has well-defined clusters, $i$ is typically small. In practice, the $k$-means algorithm is very fast. \n",
    "\n",
    "Lloyds algorithm finds a **local optimum**, not necessarily the **global optimum**\n",
    "\n",
    "Since the algorithm is fast, it is common to run the algorithm multiple times and pick the solution with the smallest total intra-cluster distance, \n",
    "$$\n",
    "\\sum_{\\ell=1}^k \\sum_{x_i \\in C_\\ell} \\| x_i - \\mu_\\ell\\|^2. \n",
    "$$\n",
    "\n",
    "* The total intra-cluster distance decreases at every iteration of Lloyd's algorithm\n",
    "\n",
    "* The total intra-cluster distance decreases with larger $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means with SciKit Learn\n",
    "\n",
    "SciKit learn has a nice [implementaton of the k-means method](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans), which we'll use to cluster various artificial datasets first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_blobs, make_moons, load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')\n",
    "# Create color maps\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap([\"#e41a1c\",\"#984ea3\",\"#a65628\",\"#377eb8\",\"#ffff33\",\"#4daf4a\",\"#ff7f00\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A first example\n",
    "\n",
    "First, we'll create data using a function that generates [gaussian blobs](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make_blobs generates gaussian blobs, we create 3 blobs\n",
    "n_samples = 500\n",
    "random_state = 170\n",
    "X, y = make_blobs(n_samples=n_samples, centers=3, random_state=random_state)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1],  marker=\"o\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now we run the scikit-learn KMeans implementation with function parameters \n",
    "* `n_clusters`: $k=3$ clusters\n",
    "* `n_init`: only one initialization \n",
    "* `init`: random initialization \n",
    "* `max_iter`: only one iteration \n",
    "\n",
    "\n",
    "\n",
    "The clusters are plotted using color. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = KMeans(n_clusters=3, n_init=1, init='random', max_iter=1).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Note:** that this is the dumbest possible version of k-means! This is essentially just random initialization + assign closest points. Try running this cell a couple of times so you see the effect of the initalization.\n",
    "\n",
    "The results are highly dependent on the random initialization of the centroids. \n",
    "\n",
    "If we set the maximum number of iterations to 5, we already see some improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = KMeans(n_clusters=3, n_init=1, init='random', max_iter=5).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### More arguments\n",
    "\n",
    "* `n_init`: Number of times the k-means algorithm will be run with different centroid seeds. The final results will be the best output of `n_init` consecutive runs in terms of the total intra-cluster distance. Defaults to 10.\n",
    "\n",
    "* `init`\n",
    " - `random` picks k random points for initial cluster seeds\n",
    " - `k-means++` (the default) uses the [k-means++](https://en.wikipedia.org/wiki/K-means%2B%2B) [algorithm](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf) to\"spread out\" the initial seeds: the first cluster center is chosen uniformly at random from the data points that are being clustered, after which each subsequent cluster center is chosen from the remaining data points with probability proportional to its squared distance from the point's closest existing cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For this simple dataset using k-means++ initalization and 10 runs of k-means,\n",
    "# even a max_iter of 1 yields good results.\n",
    "y_pred = KMeans(n_clusters=3, max_iter=1).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More complicated datasets\n",
    "\n",
    "Minimizing total intra-cluster distance (and thus the k-means algorithm) makes sense when clusters are: \n",
    "* convex\n",
    "* isotropic (uniform in all orientations). \n",
    "\n",
    "Up to now, we've applied the algorithm only to nice, Gaussian blobs of equal size. Let's see what the algorithm does for point sets that \n",
    "* vary in density and size\n",
    "* are anisotropic \n",
    "* are non-convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Density and Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with clusters that are roughly the same size in terms of how much their data points are separated, but are not equally dense: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=500, centers=3, cluster_std=1.5, random_state=17)\n",
    "\n",
    "# Unevenly density blobs\n",
    "X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10]))\n",
    "y_pred = KMeans(n_clusters=3).fit_predict(X_filtered)\n",
    "plt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);\n",
    "len(X_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means does fine for unevenly dense blobs in this case.  \n",
    "\n",
    "\n",
    "Now, let's try out blobs that are spread out more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=n_samples, centers=4, cluster_std=[0.7, 1, 3, 0.4], random_state=random_state)\n",
    "\n",
    "# Unevenly spread blobs\n",
    "y_pred = KMeans(n_clusters=3).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we seem to have some problems - because of our distance measure, some points that we'd consider as part of the \"big\" cluster are assigned to the denser clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Anisotropic point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Anisotropically distributed data\n",
    "transformation = [[ 0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "\n",
    "y_pred = KMeans(n_clusters=3).fit_predict(X_aniso)\n",
    "\n",
    "plt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred,  marker=\"o\", cmap=cmap); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a clear weakness of k-means, it doesn't work well for anisotropic point clouds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Non-convex point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_moons, y = make_moons(n_samples=n_samples, noise=.05)\n",
    "\n",
    "y_pred = KMeans(n_clusters=2).fit_predict(X_moons)\n",
    "\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The k-means method also doesn't work well for non-convex point clouds. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing k in k-means\n",
    "$k$-means is very sensitive to the choice of the parameter, $k$\n",
    "\n",
    "Here, we under-estimate *k*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=n_samples, centers=3, random_state=random_state)\n",
    "\n",
    "y_pred = KMeans(n_clusters=2).fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over-estimation is similarily problematic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=n_samples, centers=5, random_state=random_state)\n",
    "y_pred = KMeans(n_clusters=7).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**How do you choose K?** Ideas:\n",
    "\n",
    "1. Visual comparison.\n",
    "2. Looking for at which $k$ the total intra-cluster distance tapers off. \n",
    "3. [Silhouette analysis](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n",
    "\n",
    "The first method is useful when the feature vectors are lower dimensional, but it's difficult to visualize data in higher dimensions. Let's consider the other two ideas.\n",
    "\n",
    "### Analyzing Change in Intra-Cluster Distance\n",
    "\n",
    "We can run K-Means for different Ks and plot the intra-cluster distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clustering for k = 1 to k = 10\n",
    "ks = range(1,11)\n",
    "scores = []\n",
    "\n",
    "fig, axs = plt.subplots(2,5, figsize=(20, 8))\n",
    "\n",
    "for k in ks:\n",
    "    model = KMeans(n_clusters=k)\n",
    "    y_pred = model.fit_predict(X)\n",
    "    scores.append(-model.score(X))\n",
    "\n",
    "    subplot = axs[int((k-1)/5)][(k-1)%5]\n",
    "    subplot.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap, s=7);\n",
    "    subplot.set_title(\"k=\"+str(k))\n",
    "\n",
    "    \n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "plt.plot(ks, scores)\n",
    "plt.ylabel('total intra-cluster distance')\n",
    "plt.xlabel('k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We can see that the total intra-cluster distance is large for $k = 1$ and decreases as we increase $k$, until $k=5$, after which it tapers of and gets only marginally smaller. This indicates that $k=5$ is a good choice.\n",
    "\n",
    "Note: the scikit learn implementation returns negative values (\"opposites of the k-means objective\") as scores here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Silhouette Analysis\n",
    "Another method for choosing the parameter $k$ in $k$-means is called a [silhouette analysis](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html).\n",
    "\n",
    "\n",
    "In a silhouette analysis, for each point in the dataset, we compare the distance between points in the same cluster and points in neighboring clusters, using the [`silhouette_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html). \n",
    "\n",
    "The silhouette score is thus a measure of how similar an item is to its own cluster, and how different it is from other clusters. \n",
    "\n",
    "We compute it as follows. \n",
    "\n",
    "For each data element $i$ and cluster $\\ell$, let $d(i,\\ell)$ be the mean distance between $i$ and the points in cluster $\\ell$. We interpret $d(i,\\ell)$ as the disimilarity of $i$ with the items in cluster $\\ell$: \n",
    "+ If $d(i,\\ell)$ is small, then $i$ is very similar to the items in cluster $\\ell$. \n",
    "+ If $d(i,\\ell)$ is larger, then $i$ is very different from the items in cluster $\\ell$.  \n",
    "\n",
    "For each data element $i$, we define the *mean inter-cluster distance*\n",
    "$$\n",
    "a_i = d(i,\\ell_i),\n",
    "$$\n",
    "where $\\ell_i$ denotes the cluster that contains element $i$. Thus, $a_i$ measures how well element $i$ is assigned to its own cluster, with a small number being better. \n",
    "\n",
    "For each data element $i$, we define $b_i$ to be the *mean nearest-cluster distance*, that is, the smallest distance $d(i,\\ell)$, where we only consider clusters $\\ell$ for which $i$ is not a member:\n",
    "$$\n",
    "b_i = \\min_{ \\ell \\colon \\ell \\neq \\ell_i} d(i,\\ell). \n",
    "$$\n",
    "\n",
    "The *silhouette coefficient* for data element $i$ is then given by \n",
    "$$ \n",
    "s_i = \\frac{b_i - a_i}{\\max\\{a_i, b_i\\} }.\n",
    "$$\n",
    "The silhouette coefficient has a range of $[-1, 1]$ with 1 being good. **Why? What would cause a negative value?**\n",
    "\n",
    "The *silhouette score* is the mean silhouette coefficient of all samples, \n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n s_i.\n",
    "$$\n",
    "\n",
    "\n",
    "There is a nice plot, called a *silhouette plot*, which shows the silhouette coefficient for each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette coefficient of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some simpler code which just calculates and plots silhouette scores\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "silhouette_avg = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "    silhouette_avg.append(silhouette_score(X, cluster_labels))\n",
    "    \n",
    "print(silhouette_avg)    \n",
    "plt.plot(range_n_clusters,silhouette_avg)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Iris dataset\n",
    "\n",
    "We're going to look at how well k-means does on a higher dimensional example, the well-known Iris dataset. \n",
    "\n",
    "First, let's load the dataset and look at the first two dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y_iris_gt = iris.target\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y_iris_gt, cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This isn't very discernible in just two dimensions. We'll see how we can better plot this once we've talked about Principle Component Analysis (PCA). \n",
    "\n",
    "Now let's run a clustering algorithm using all features (dimensions) and plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_iris_pred = KMeans(n_clusters=3,n_init=100).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_iris_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "These clusters looks similar to the ground truth labels.\n",
    "\n",
    "How do we evaluate the quality of the clusters? \n",
    "\n",
    "Just comparing the labels is difficult, since permutations of the labels shouldn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(y_iris_pred)\n",
    "print(Y_iris_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Evaluating cluster quality with known ground-truth labels\n",
    "\n",
    "1. [`homogeneity_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html): Homogeneity metric of a cluster labeling given a ground truth. A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class. Bad homogeneity tends to mean we have under-clustered, i.e. $k$ too small.\n",
    "+ [`completeness_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html): A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster. Bad completeness tends to mean we have over-clustered, i.e. $k$ too large.\n",
    "+ [`v_measure_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html): The V-measure is the harmonic mean between homogeneity and completeness:\n",
    "$$\n",
    "v = 2 * (homogeneity * completeness) / (homogeneity + completeness)\n",
    "$$\n",
    "+ [`homogeneity_completeness_v_measure`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html): Compute the homogeneity, completeness, and v-Measure scores at once.\n",
    "+ Confusion matrix and purity score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.homogeneity_score([0,0,1,1],[1,1,2,2]))\n",
    "print(metrics.homogeneity_score([0,0,1,1],[0,0,0,1]))\n",
    "# Compare this with some of our previous metrics for classification- here the labelings must match\n",
    "print(metrics.f1_score([0,0,1,1],[1,1,0,0]))\n",
    "print(metrics.f1_score([0,0,1,1],[0,0,1,1]))\n",
    "print(metrics.recall_score([0,0,1,1],[1,1,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "metrics.homogeneity_completeness_v_measure(Y_iris_gt,y_iris_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The homogeneity, completeness, and v-meausre scores are all approximately 76%. This is ok, but not great. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
