{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science – Basic Practical Natural Language Processing (NLP)\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/* \n",
    "\n",
    "In this lecture, we introduce some practical NLP following up on the theoretical lecture. We will do some basic text processing followed by analyzing sentiment for movie reviews. For this purpose, we'll introduce  the [Natural Language Toolkit (NLTK)](http://www.nltk.org/), a Python library for natural language processing. \n",
    "\n",
    "We won't cover NLTK or NLP extensively here – this lecture is meant to give you a few pointers if you want to use NLP in the future, e.g., for your project.\n",
    "\n",
    "Also, there is a well-regarded alternative to NLTK: [Spacy](https://spacy.io/). If you're planning to use a lot of NLP in your project, that might be worth checking out. \n",
    "\n",
    "If you're interested in using LLMs, you may want to try [LangChain](https://python.langchain.com/docs/get_started/). Note for most cases, it needs to be hooked up to a *paid* API. You can run some models locally, but they require a capable GPU.\n",
    "\n",
    "**Reading:** \n",
    "\n",
    "[S. Bird, E. Klein, and E. Loper, *Natural Language Processing with Python – Analyzing Text with the Natural Language Toolkit*](http://www.nltk.org/book/). \n",
    "\n",
    "\n",
    "[C. Manning and H. Schütze, *Foundations of Statistical Natural Language Processing* (1999).](http://nlp.stanford.edu/fsnlp/)\n",
    "\n",
    "[D. Jurafsky and J. H. Martin, *Speech and Language Processing* (2016).](https://web.stanford.edu/~jurafsky/slp3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next week:** guest lecturer Dr. Vivek Srikumar will be speaking about state-of-the-art techniques!   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Tasks\n",
    "\n",
    "There are several tasks we might perform in understand text data:\n",
    "* Part of speech tagging (what are the nouns, verbs, adjectives, prepositions).\n",
    "+ Information Extraction\n",
    "+ Sentiment Analysis (determine the attitude of text, e.g., is it positive or negative).\n",
    "+ Semantic Parsing (translate natural language into a formal meaning representation).\n",
    "\n",
    "The current strategy for many NLP tasks is to find a good way to represent the text (\"extract features\") and then to use machine learning / statistics tools, such as classification or clustering. \n",
    "\n",
    "Our goal today is to use NLTK + scikit-learn to do some basic NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install datasets and models\n",
    "\n",
    "To use NLTK, you must first download and install the datasets and models. The following cell does this. It will take some time, so be careful when you clear outputs or re-run cells. It's better if you can preserve this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note this part can take some time! Try to run it only once.\n",
    "# Be careful when you clear outputs so you don't need to re-run it.\n",
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 9)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of NLTK\n",
    "\n",
    "We have downloaded a set of text corpora above. Here is a list of these texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 20 words of text1 – Moby Dick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Statistics\n",
    "\n",
    "We can check the length of a text. The text of Moby Dick is 260,819 words, whereas Monty Python and the Holy Grail has 16,967 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check for the frequency of a word. The word \"swallow\" appears 10 times in Monty Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text6.count(\"swallow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to know the context in which \"swallow\" appears in the text\n",
    "\n",
    "\"You shall know a word by the company it keeps.\" – John Firth\n",
    "\n",
    "Use the [`concordance`](http://www.nltk.org/api/nltk.html#nltk.text.Text.concordance) function to print out the words just before and after all occurrences of the word \"swallow\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text6.concordance(\"swallow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that occur with notable frequencey are \"fly\" or \"flight\", \"unladen\", \"air\", \"African\", \"European\". We can learn about what a swallow can do or properties of a swallow by this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we look for Ishmael in Moby Dick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance(\"Ishmael\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see a lot of \"I\"s. We could probably infer that Ishmael is the narrator based on that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what other words frequently appear in the same context using the  [`similar`](http://www.nltk.org/api/nltk.html#nltk.text.Text.similar) function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text6.similar(\"swallow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text6.similar(\"african\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text6.similar(\"coconut\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that 'african' and 'unladen' both appeared in the text with the same word just before and just after. To see what the phrase is, we can use the [`common_contexts`](http://www.nltk.org/api/nltk.html#nltk.text.Text.concordance) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text6.common_contexts([\"African\", \"unladen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both \"an unladen swallow\" and \"an african swallow\" appear in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text6.concordance(\"unladen\")\n",
    "print()\n",
    "text6.concordance(\"african\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion plot\n",
    "\n",
    "`text4` is the Inaugural Address Corpus which includes inaugural addresses going back to 1789. \n",
    "We can use a dispersion plot to see where in a text certain words appear, and hence how the language of the address has changed over time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add back in the spaces so we can read it first.\n",
    "print(\" \".join(text4[:100]))\n",
    "print(\"\")\n",
    "\n",
    "print(\" \".join(text4[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Based on the above, let's see how some words have changed over time.\n",
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duty\", \"America\", \"nation\", \"God\", \"military\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring texts using statistics\n",
    "\n",
    "We'll explore a text by counting the frequency of different words.\n",
    "\n",
    "The total number of words (\"outcomes\") in Moby Dick is 260,819 and the number of unique words (\"samples\") is 19,317. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_dist = FreqDist(text1)\n",
    "print(frequency_dist)\n",
    "\n",
    "# find 50 most common words\n",
    "print('\\n',frequency_dist.most_common(50))\n",
    "\n",
    "# not suprisingly, whale occurs quite frequently (906 times!)\n",
    "print('\\n', frequency_dist['whale'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find all the words in Moby Dick with more than 15 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set(text1)\n",
    "long_words = [w.lower() for w in unique_words if len(w) > 15]\n",
    "long_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "\n",
    "Often, it is useful to ignore frequently used words, to concentrate on the meaning of the remaining words. These are referred to as *stopwords*. Examples are \"the\", \"was\", \"is\", etc. \n",
    "\n",
    "NLTK comes with a stopword corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the task, these stopwords are important modifiers, or superfluous content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Frequent Words\n",
    "Find the most frequently used words in Moby Dick that are not stopwords and not punctuation. Hint: [`str.isalpha()`](https://docs.python.org/3/library/stdtypes.html#str.isalpha) could be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords in different corpora\n",
    "Is there a difference between the frequency in which stopwords appear in the different texts? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return len(content) / len(text)\n",
    "\n",
    "for i,t in enumerate([text1,text2,text3,text4,text5,text6,text7,text8,text9]):\n",
    "    print(i+1,content_fraction(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, \"text8: Personals Corpus\" has the most content. Why is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(text8[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations\n",
    "A *collocation* is a sequence of words that occur together unusually often, we can retreive these using the [`collocations()`](http://www.nltk.org/api/nltk.html#nltk.text.Text.collocations) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis for movie reviews\n",
    "When analyzing movie reviews, we can ask the simple question: Is the attitude of a movie review positive or negative? If you're developing [rotten tomatoes](https://www.rottentomatoes.com/), that's what you want to know to certify whether a review is \"fresh\" or \"rotten\".\n",
    "\n",
    "How can we approach this question?\n",
    "\n",
    "Our data is a corpus consisting of 2000 movie reviews together with the user's sentiment polarity (positive or negative). More information about this dataset is available [from this website](https://www.cs.cornell.edu/people/pabo/movie-review-data/).\n",
    "\n",
    "Our goal is to predict the sentiment polarity from just the review. \n",
    "\n",
    "Of course, this is something that we can do very easily: \n",
    "1. That movie was terrible. -> negative\n",
    "+ That movie was great! -> positive\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datset contains 1000 positive and 1000 negative movie reviews. \n",
    "\n",
    "The paths to / IDs for the individual reviews are accessible via the fileids() call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.fileids()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the positives or negatives explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.fileids('pos')[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are in fact 1000 positive and 1000 negative reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reviews = len(reviews.fileids())\n",
    "print(num_reviews)\n",
    "print(len(reviews.fileids('pos')),len(reviews.fileids('neg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the review for the third movie. Its a negative review for [The Mod Squad](https://www.rottentomatoes.com/m/mod_squad/) (see the [trailer](https://www.youtube.com/watch?v=67cdXuWnRKs)), which has a \"rotten\" rating on rotten tomatoes. \n",
    "\n",
    "![Mod Squad at Rotten Tomatoes](mod_squad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the name of the file \n",
    "fid = reviews.fileids()[2]\n",
    "print(fid)\n",
    "\n",
    "print('\\n', reviews.raw(fid))\n",
    "\n",
    "\n",
    "print('\\n', \"The Category:\", reviews.categories(fid) )\n",
    "\n",
    "print('\\n', \"Individual Words:\",reviews.words(fid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some sentences that indicate that this is a negative review:\n",
    "\n",
    " * \"it is movies like these that make a jaded movie viewer thankful for the invention of the timex indiglo watch\"\n",
    " * \"sounds like a cool movie , does it not ? after the first fifteen minutes , it quickly becomes apparent that it is not .\" \n",
    " * \"nothing spectacular\"\n",
    " * \"avoid this film at all costs\"\n",
    " * \"unfortunately , even he's not enough to save this convoluted mess\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Custom Algorithm\n",
    "We'll build a sentiment classifier using methods we already know to predicts the label ['neg', 'pos'] from the review text\n",
    "\n",
    "`reviews.categories(file_id)` returns the label ['neg', 'pos'] for that movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [reviews.categories(fid) for fid in reviews.fileids()]\n",
    "print(categories[0:10])\n",
    "labels = {'pos':1, 'neg':0}\n",
    "# create the labels: 1 for positive, 0 for negative\n",
    "y = [labels[x[0]] for x in categories]\n",
    "# output labels for the first (a negative) and the 1000th (a positive review)\n",
    "y[0], y[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we collect all words into a nested array datastructure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_words = [list(reviews.words(fid)) for fid in reviews.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 10 words of the third document - mod squad\n",
    "doc_words[2][1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get all of the words in the reviews and make a FreqDist, pick the most common 2000 words and remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the 2000 most common words in lowercase\n",
    "most_common = nltk.FreqDist(w.lower() for w in reviews.words()).most_common(2000)\n",
    "\n",
    "# remove stopwords\n",
    "filtered_words = [word_tuple for word_tuple in most_common if word_tuple[0].lower() not in stopwords]\n",
    "# remove punctuation marks\n",
    "filtered_words = [word_tuple for word_tuple in filtered_words if word_tuple[0].isalpha()]\n",
    "print(len(filtered_words))\n",
    "filtered_words[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  extract this word list from the frequency tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features =  [word_tuple[0] for word_tuple in filtered_words]\n",
    "print(word_features[:5])\n",
    "len(word_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that takes a document and returns a list of zeros and ones indicating which of the words in  `word_features` appears in that document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    # convert each document into a set of its words \n",
    "    # this removes duplicates and makes \"existence\" tests efficient\n",
    "    document_words = set(document)\n",
    "    # a list, initalized with 0s, that we'll set to 1 for each of the words that exists in the document\n",
    "    features = np.zeros(len(word_features))\n",
    "    for i, word in enumerate(word_features):\n",
    "        features[i] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just focus on the third document. Which words from `word_features` are in this document? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_doc_2 = document_features(doc_words[2])\n",
    "print(words_in_doc_2)\n",
    "\n",
    "inds = np.where(words_in_doc_2 == 1)[0]\n",
    "print('\\n', [word_features[i] for i in inds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build our feature set for all the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros([num_reviews,len(word_features)])\n",
    "for i in range(num_reviews):\n",
    "    X[i,:] = document_features(doc_words[i])\n",
    "\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a feature vector for each of these reviews that we can use in classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have features for each document and labels, **we have a classification problem!** \n",
    "\n",
    "NLTK has a built-in classifier, but we'll use the scikit-learn classifiers we're already familiar with. \n",
    "\n",
    "Let's try k-nearest neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30\n",
    "model = KNeighborsClassifier(n_neighbors=k)\n",
    "scores = cross_val_score(model, X, y, cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='rbf', C=30, gamma=\"auto\")\n",
    "scores = cross_val_score(model, X, y, cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that kNN with these parameters is less accurate than SVM, which is about 80% accurate. Of course, we could now use cross validation to find the optimal parameters, `k` and `C`, but as always, SVM is slow... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's see what our algorithm things about the Mod Squad! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y, random_state=1, test_size=0.2)\n",
    "\n",
    "model.fit(XTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_squad = [X[2]]\n",
    "mod_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(mod_squad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model says 0 - so a bad review! We have succesfully build a classifier that can detect the Mod Squad review as a bad review! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a mis-classified movie. Remember, that the first 1000 movies are negative reviews, so we can just look for the first negative one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review 9, which was misclassified, is for Aberdeen, which has [generally favorable reviews](https://www.rottentomatoes.com/m/aberdeen/) with about 80% positive. Let's looks at the review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = reviews.fileids()[8]\n",
    "\n",
    "print('\\n', reviews.raw(fid))\n",
    "print('\\n', reviews.categories(fid) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we read this, we can see that this is a negative review, but not a terrible review. Take this sentence for example: \n",
    "\n",
    " * \"if signs & wonders sometimes feels overloaded with ideas , at least it's willing to stretch beyond what we've come to expect from traditional drama\"\n",
    " * \"yet this ever-reliable swedish actor adds depth and significance to the otherwise plodding and forgettable aberdeen , a sentimental and painfully mundane european drama\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We could have also used the Classifier from the NLTK library\n",
    "\n",
    "Below is the sentiment analysis from [Ch. 6 of the NLTK book](http://www.nltk.org/book/ch06.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(list(reviews.words(fileid)), category)\n",
    "             for category in reviews.categories() \n",
    "             for fileid in reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list contains tuples where the review, stored as an array of words, is the first item in the tuple and the category is the second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the features from all of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):    \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains('+ word +')'] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train_set, test_set and perform classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK gives us 88% accuracy, which isn't bad, but our home-made naive algorithm also achieved a respectable 80%.\n",
    "\n",
    "\n",
    "What improvements could we have made? Obviously, we could have used more data, or – in our home-grown model select words that discriminate between good and bad reviews. We could have used n-grams, e.g., to catch \"not bad\" as a postitive sentiment."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
