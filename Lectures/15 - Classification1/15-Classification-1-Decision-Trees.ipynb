{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Data Science – Lecture 15: Classification 1 – Decision Trees\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "\n",
    "\n",
    "You should install these two libraries to draw decision trees:\n",
    "* PyDotPlus: `conda install pydotplus`\n",
    "* GraphViz: Go [here](https://www.graphviz.org/download/) to download the app. On Mac and with homebrew you can run `brew install graphviz`. On Windows and with conda you can run `conda install graphviz`. \n",
    "\n",
    "Recommended Reading: \n",
    "* G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning, Ch. 8 [digitial version available here](https://www.statlearning.com/)\n",
    "* [Visual Intro to Machine Learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "\n",
    "This lecture is in part based on [this lab](https://github.com/cs109/2015lab7/blob/master/Lab7-Botany%20and%20Ensemble%20Methods.ipynb), [this blog](http://hamelg.blogspot.com/2015/11/python-for-data-analysis-part-29.html), and the [scikit learn documentation](http://scikit-learn.org/stable/modules/tree.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "We'll consider [a dataset of Titanic passengers](https://www.kaggle.com/c/titanic/) and develop a model to predict whether a particular passenger will survive or not. This is what [the dataset](titanic.csv) looks like:\n",
    "\n",
    "\n",
    "| PassengerId | *Survived* | Pclass | Name                                                                               | Sex    | Age  | SibSp | Parch | Ticket             | Fare     | Cabin           | Embarked |\n",
    "|-------------|----------|--------|------------------------------------------------------------------------------------|--------|------|-------|-------|--------------------|----------|-----------------|----------|\n",
    "| 1           | 0        | 3      | Braund, Mr. Owen Harris                                                            | male   | 22   | 1     | 0     | A/5 21171          | 7.25     |                 | S        |\n",
    "| 2           | 1        | 1      | Cumings, Mrs. John Bradley (Florence Briggs Thayer)                                | female | 38   | 1     | 0     | PC 17599           | 71.2833  | C85             | C        |\n",
    "| 3           | 1        | 3      | Heikkinen, Miss. Laina                                                             | female | 26   | 0     | 0     | STON/O2. 3101282   | 7.925    |                 | S        |\n",
    "| 4           | 1        | 1      | Futrelle, Mrs. Jacques Heath (Lily May Peel)                                       | female | 35   | 1     | 0     | 113803             | 53.1     | C123            | S        |\n",
    "| 5           | 0        | 3      | Allen, Mr. William Henry                                                           | male   | 35   | 0     | 0     | 373450             | 8.05     |                 | S        |\n",
    "\n",
    "\n",
    "Here are the variable descriptions for the non-obvious variables:\n",
    " * **Survived:**        (0 = No; 1 = Yes)\n",
    " * **pclass:**          Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    " * **sibsp:**           Number of Siblings/Spouses Aboard\n",
    " * **parch:**           Number of Parents/Children Aboard\n",
    " * **ticket:**          Ticket Number\n",
    " * **fare:**            Passenger Fare\n",
    " * **cabin:**           Cabin\n",
    " * **embarked:**        Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "\n",
    "We're trying to decide whether a passenger with particular attributes will survive the Titanic disaster, so the **survival** variable is our label, the other columns are the **features**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "[Decision trees](https://en.wikipedia.org/wiki/Decision_tree_learning) are very intuitive classification and regression tools. \n",
    "\n",
    "Consider the following [decision tree from the New York Times](https://archive.nytimes.com/www.nytimes.com/imagepages/2008/04/16/us/20080416_OBAMA_GRAPHIC.html), \n",
    "which predicts whether a county voted for Obama or Clinton in the 2008 democratic primary. \n",
    "\n",
    "<img src=\"oc-tree.jpeg\" width=\"500\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Here is another decision tree for the survival of passengers on the titanic.  The figures under the leaves show the probability of survival and the percentage of observations in the leaf.\n",
    "\n",
    "![](titanic_tree.png)\n",
    "\n",
    "The use of a decision tree is very simple. Suppose someone gives you this tree and a new person. In order to predict whether or not the person would have died on the titanic, you ask the following questions, in order:\n",
    "\n",
    " * Is the person male? If no, we predict they would have survived. If yes, continue.\n",
    " * Is the person older than 9.5 years? If yes, we predict they would have died. If no, continue.\n",
    " * Did the person have 3 or more siblings? If yes, we predict they would have died. If no, they would have survived.\n",
    " \n",
    "Many of these decisions make intuitive sense: social convention gave women and children preference for rescue. Some, however, are not as obvious: why is having more than 3 siblings or spouses a marker? Could this be a marker for a hidden variable?\n",
    "\n",
    "The question we'll move to now is: How would one create such a tree? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a Decision Tree\n",
    "\n",
    "Building a decision tree isn't really much harder than reading one. Here's the essential rundown of the [ID3 algorithm](https://en.wikipedia.org/wiki/ID3_algorithm):\n",
    "\n",
    " * If the data all have the same label, create a leaf node that predicts that label, and you're done.\n",
    " * If the list of attributes is empty (e.g., because you have used all already), create a leaf node that predicts the most common label.\n",
    " * Else, partition the data by each of the attributes; choose the attribute/partition with the lowest error.\n",
    " * Recursively continue on each partitioned subset using the remaining attributes.\n",
    " * Terminate when no attributes are left (see above), or when desired depth or another termination criterion is reached.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is a very nice, [interactive tutorial on decision trees](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/) that considers the problem of predicting whether a home is in *San Francisco* or *New York* based on attributes.\n",
    "![](http://www.r2d3.us/static/pages/decision-trees-part-1/preview-en.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, how do you choose the partition with the lowest error? There are various approaches.\n",
    "\n",
    "Let's say we're building a classification tree by considering a list of predictors. In our example above we want to be able to classify whether people have survived based on things like gender, age, the booked fare, etc. Let's call the variables $X_{i,p}$ ($i$ for passengers, $p$ for predictors). Initially, for the first split, we consider all the passengers and all the predictors. We also have an observed label $Y_i$ for each passenger, and a predicted label $\\hat{Y}_i$. \n",
    "\n",
    "We can calculate the *mean error*, \n",
    "$$\n",
    "ME = \\frac{1}{N}\\sum_{i=1}^N \\ell(\\hat{Y}_i, Y_i),\n",
    "$$ \n",
    "where $\\ell(\\hat{Y}_i, Y_i)$ is the error for sample $i$. Here, the error would be \n",
    "* squared error for regression, *i.e.*, $\\ell(\\hat{Y}_i, Y_i) = (\\hat{Y}_i - Y_i)^2$ and \n",
    "* either [*cross-entropy*](https://en.wikipedia.org/wiki/Cross_entropy) \n",
    "or \n",
    "[*Gini impurity*](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) \n",
    "for classification. \n",
    "\n",
    "\n",
    "We want to achieve two things: pick the **best split** for the **best predictor**.\n",
    "\n",
    "- At **each step** of the algorithm we consider a list of possible decisions or splits, *e.g.*, $X_{i,6} > 9$ (age is greater than 9), or $X_{i,5} = female$.\n",
    "- For each possible decision we recalculate the predictor for that rule, for example $\\hat{Y}_i = 1$ if $X_{i,6} > 9$ and $0$ otherwise.\n",
    "- We recalculate the mean error for each possible decision\n",
    "- We choose the decision that reduces the error by the largest amount.\n",
    "- Then continue with the next step on the reduced input set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In building decision trees, it is easy to overfit the data. There are several methods for avoiding this, which we'll discuss more below. Simple strategies include limiting the depth of a tree or only splitting when we have more than $N$ samples left. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees with SciKit Learn\n",
    "\n",
    "Scikit-learn has [a nice decision tree implementation](http://scikit-learn.org/stable/modules/tree.html) which we'll use to learn a tree for our Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"titanic.csv\")\n",
    "titanic.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data cleanup\n",
    "\n",
    "We need to do some cleanup: \n",
    " * Age has missing values. Let's assume that a person with missing age is of mean age (this is not necessarily a good decision).\n",
    " * Embarked has missing values, we add a dedicated category for unknown embarkation points.\n",
    " * We need to convert the categorical values to numerical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].mean())\n",
    "\n",
    "def sex_to_numeric(x):\n",
    "    if x=='male':\n",
    "        return 0\n",
    "    if x=='female':\n",
    "        return 1\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "titanic[\"Sex\"] = titanic[\"Sex\"].apply(sex_to_numeric)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this will break if run more than once\n",
    "def embarked_to_numeric(x):\n",
    "    if x==\"S\":\n",
    "        return 0\n",
    "    if x==\"C\":\n",
    "        return 1\n",
    "    if x==\"Q\":\n",
    "        return 2\n",
    "    else: \n",
    "        return 3\n",
    "    \n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].apply(embarked_to_numeric)\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's work only with the numerical and categorical variables and omit passengerID, Name, Ticket and Cabin. These values could contain some information, but it's hard to make sense of them without more context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "titanic = titanic[features]\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring the data\n",
    "\n",
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "1. What percentage of passengers survived? \n",
    "2. What percentage of passengers are female?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.pairplot(titanic, hue=\"Survived\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "It's difficult to find clear patterns here, except for the separation of Sex.\n",
    "\n",
    "Here is a better plot, with a visualization technique called [parallel sets](http://bl.ocks.org/igorzilla/3086651):\n",
    "\n",
    "![Parallel Sets](p_sets.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Our first tree\n",
    "Here is some code that splits the data into training and test sets for cross-validation and selects features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#labels =[\"Survived\", \"Perished\"]\n",
    "\n",
    "def splitData(features):\n",
    "    \"\"\"Split a subset of the titanic dataset, given by the features, into train and test sets.\"\"\"\n",
    "    titanic_predictors = titanic[features].values\n",
    "    titanic_labels = titanic[\"Survived\"].values\n",
    "\n",
    "    # Split into training and test sets\n",
    "    XTrain, XTest, yTrain, yTest = train_test_split(titanic_predictors, titanic_labels, random_state=1, test_size=0.5)\n",
    "    return XTrain, XTest, yTrain, yTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "And more code for plotting decision trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display  \n",
    "import pydotplus \n",
    "from scipy import misc\n",
    "\n",
    "def renderTree(my_tree, features):\n",
    "    # hacky solution of writing to files and reading again\n",
    "    # necessary due to library bugs\n",
    "    filename = \"temp.dot\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f = tree.export_graphviz(my_tree, \n",
    "                                 out_file=f, \n",
    "                                 feature_names=features, \n",
    "                                 class_names=[\"Perished\", \"Survived\"],  \n",
    "                                 filled=True, \n",
    "                                 rounded=True,\n",
    "                                 special_characters=True)\n",
    "  \n",
    "    dot_data = \"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        dot_data = f.read()\n",
    "\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    image_name = \"temp.png\"\n",
    "    graph.write_png(image_name)  \n",
    "    display(Image(filename=image_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now let's look at a decision tree that **ONLY operates on sex**! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "decisionTree = tree.DecisionTreeClassifier()\n",
    "\n",
    "XTrain, XTest, yTrain, yTest = splitData([\"Sex\"])\n",
    "# fit the tree with the traing data\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "# predict with the training data\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "# measure accuracy\n",
    "print('Accuracy on training data = ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "# predict with the test data\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "# measure accuracy\n",
    "print('Accuracy on test data = ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n",
    "\n",
    "renderTree(decisionTree, [\"Sex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "~76% correct on the test set isn't bad! - sex seems to be a very good indicator of whether someone has survived or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adding more features\n",
    "\n",
    "Let's add the number of siblings and spouses as a feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  train tree on sex and the number of the number of siblings/spouses aboard\n",
    "used_features = [\"Sex\", \"SibSp\"]\n",
    "XTrain, XTest, yTrain, yTest = splitData(used_features)\n",
    "decisionTree = tree.DecisionTreeClassifier()\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n",
    "renderTree(decisionTree, used_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Our accuracy on the training data has gone up, **but the accuracy on the test data has gone down**. It looks like we're overfitting. But maybe we just selected the wrong features? Let's just try all of them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "XTrain, XTest, yTrain, yTest = splitData(all_features)\n",
    "decisionTree = tree.DecisionTreeClassifier()\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n",
    "renderTree(decisionTree, all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "OK, clearly, we're overfitting the data - 98% accuracy on the training data and only ~75% on the test data. Yet, we've created a complicated tree. \n",
    "\n",
    "Decision trees are notorious for overfitting the data. There are two parameters that help us reign in overfitting:\n",
    "\n",
    "* **max_depth:** The maximum depth of the tree. If this is not set, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "* **min_samples_split:** The minimum number of samples required to split an internal node: If the value is an integer, then consider `min_samples_split` as the minimum number. If it is float, then `min_samples_split` is a percentage and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split.\n",
    "\n",
    "Let's try combinations of depth and min split size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Limiting Depth!\n",
    "decisionTree = tree.DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n",
    "renderTree(decisionTree, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lmiting the minimum samples used to split.\n",
    "decisionTree = tree.DecisionTreeClassifier(min_samples_split=15)\n",
    "\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n",
    "renderTree(decisionTree, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Limiting Both\n",
    "\n",
    "decisionTree = tree.DecisionTreeClassifier(max_depth=3, min_samples_split=10)\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n",
    "renderTree(decisionTree, all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "It looks like both, the minimum number of samples for splitting and the maximum depth help with overfitting and we achieve a 79-80% accuracy rate. That doesn't sound much better than just gender alone, but 4% improvement is a lot in classification.\n",
    "\n",
    "Also, our last model is fairly simple yet quite accurate. The main point seems to be:\n",
    "\n",
    " * Sex is the dominant factor at the root of the tree\n",
    " * For females, if you're in class 1 or 2 you're almost sure to survive\n",
    " * For males, if you were younger than 6.5 years old, you had a chance to survive. \n",
    " * Also note that there are branches that predict the same thing, but with different certainty. For example, in the male / adult category, if you were in \"first class\", you still were likely to die, but less likely than in second or third. \n",
    " \n",
    "Of course, here, we made these decisions on how to limit the tree manually. What would be a good approach to do this automatically? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion \n",
    "\n",
    "### Advantages of decision trees\n",
    "* Decision trees are simple to explain and interpret, and interpretability is a major issue in many applications. Think about credit decisions, or medical decisions. \n",
    "* There is a nice graphical display for trees\n",
    "* Easy to handle categorical predictors, though with a caveat: SciKit Learn doesn't really support it (see \"embarked\" above). An alternative is to use [one-hot-encoding](https://en.wikipedia.org/wiki/One-hot); i.e., for each category you create a binary dimension \"is in category or not\". \n",
    "\n",
    "### Disadvantages of decision trees\n",
    "* Decision trees generally don't have the predictive accuracy of other approaches as they tend to overfit the data. \n",
    "* Decision trees are non-robust, *i.e.*, sensitive to small changes in the data.\n",
    "\n",
    "Both of these disadvantages are addressed by the following, more advanced methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble Methods based on Decision Trees\n",
    "\n",
    "Ensemble Methods (Ensemble Learning) use multiple algorithms at the same time and then come to a consensus of a predictive label. You can read a little about such methods in ISL, Ch. 8.\n",
    "\n",
    "One such method is **Bagging** (Bootstrap Aggregating). The idea of [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating) is to generate several trained models (e.g., decision trees) based on subsets of the data and let the decision trees vote to arrive at a prediction.\n",
    "Commonly the subset is chosen through bootstrapping, i.e., random sampling with replacement. \n",
    "Since averaging a set of observations reduces variances (think CLT), this increases the predictive accuracy of the method. \n",
    "\n",
    "\n",
    "[**Boosting**](https://en.wikipedia.org/wiki/Boosting_(machine_learning)) is similar to bagging, except that the trees are grown sequentially and are trained specifically to address previously mis-classified items.  \n",
    "\n",
    "When applied to decision trees, these methods are called [Random Forests](https://en.wikipedia.org/wiki/Random_forest). Generally, random forests combine multiple decision trees that were generated with some randomness and let them vote on the result. Here the randomness comes from choosing a random sample of the prediction variables to build each tree. \n",
    "\n",
    "\n",
    "Here is an example with a random forest that uses *300* trees and bootstraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest =  RandomForestClassifier(bootstrap=True, n_estimators=300, random_state=0)\n",
    "\n",
    "trained_forest = forest.fit(XTrain, yTrain)\n",
    "\n",
    "y_pred_train = trained_forest.predict(XTrain)\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "y_pred = trained_forest.predict(XTest)\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods are generally of higher accuracy, but also take a lot longer to train. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
