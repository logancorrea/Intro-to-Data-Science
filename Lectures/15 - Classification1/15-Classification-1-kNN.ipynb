{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Data Science – Lecture 15: Classification 1\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "In this lecture, we'll discuss:\n",
    "* a general overview of classification  \n",
    "* k nearest neighbors (k-NN)\n",
    "* decision trees \n",
    "* generalizability and cross validation \n",
    "\n",
    "Recommended Reading: \n",
    "* G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning, Ch. 4 [digital version available here](https://www.statlearning.com/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors \n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_moons, load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification \n",
    "\n",
    "Recall that in **regression**, we try to predict a real-valued (quantitative) variable. Examples:\n",
    "* Predict house prices based on attributes\n",
    "* Predict credit score rating based on income, balance, gender, education, etc...\n",
    "\n",
    "In **classification**, we try to predict a categorical (qualitative) variable. Examples:\n",
    "* Predict whether a bank should issue a person a credit card (yes/no)\n",
    "* Predict a hospital patient's diagnosis (stroke, heart attack,...) based on symptoms. \n",
    "\n",
    "We assume a dataset with $n$ samples $(x_1,y_1), (x_2,y_2),\\ldots,(x_n,y_n)$, where $x_i$ are attributes or features and $y_i$ are categorical variables that you want to predict. \n",
    "\n",
    "Our Goal is to develop a rule for predicting the categorical variable $y$ based on the features $x$. \n",
    "\n",
    "For example, can we predict whether or not a student will be admitted to a graduate program based on their undergraduate performance (GPA, GRE score, prestige of student's undergraduate university)? \n",
    "\n",
    "Or, for example, the post office uses classification of hand-written zip codes to automatically sort mail. The digits of the zip code are photographed, the picture serves as your data vector, and the picture is then assigned to one of the *classes:* $0,1,2,\\ldots,9$. \n",
    "\n",
    "In the next couple lectures, we'll cover several classification methods:\n",
    "* k-nearest neighbors\n",
    "* trees and random forests\n",
    "* logistic regression (time permitting)\n",
    "* support vector machines (SVM) \n",
    "* Neural Networks and Deep Learning\n",
    "\n",
    "We'll see that Neural Networks can be used for both regression and classification. \n",
    "\n",
    "Today we will be covering k-nearest neighbors and decision trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K–Nearest Neighbors (k-NN) \n",
    "\n",
    "**Idea:** To decide the class of a given point, find the k nearest neighbors of that point, and let them \"vote\" on the class. That is, we assign the class to the sample that is most common among its k nearest neighbors. \n",
    "\n",
    "**Considerations:**\n",
    "* We must pick k, the number of voting neighbors (typically a small number, say k=10)\n",
    "* 'Nearest' means closest in distance, so there is some flexibility in defining the distance\n",
    "* There are different ways to vote. For example, of the k nearest neighbors, I might give the closest ones more weight than farther ones. \n",
    "* We have to decide how to break ties in the vote. \n",
    "* How to find nearest neighbors efficiently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Classifying Two Moons\n",
    "\n",
    "Let's consider a synthetic dataset in the shape of \"two moons\". Here, each sample has two pieces of information: \n",
    "* the *features*, denoted by $x_i$, which are just a two-dimensional coordinate and \n",
    "* a *class*, denoted by $y_i$, which is either 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# there are two features contained in X and the labels are contained in y\n",
    "X,y = make_moons(n_samples=500,random_state=1,noise=0.3)\n",
    "\n",
    "# X is a 500x2 numpy.ndarray containing the coordinates for each sample\n",
    "# y is a 500x1 numpy.ndarray containing the class for each sample\n",
    "print(np.shape(X), np.shape(y))\n",
    "print(X[:10])\n",
    "print(y[:10])\n",
    "\n",
    "# Plot the data, color by class\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"OrangeRed\", marker=\"o\",label=\"0\")\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"SteelBlue\", marker=\"s\",label=\"1\")\n",
    "\n",
    "test1 = [1.5,-1] # should be 1\n",
    "test2 = [-1,1] # should be 0\n",
    "test3 = [0,0.1] # borderline, probably 1\n",
    "test4 = [0,0.5] # borderline, probably 0 \n",
    "test5 = [0.5,0.75] # borderline, probably 0 \n",
    "test = [test1, test2, test3, test4, test5]\n",
    "\n",
    "plt.scatter([x[0] for x in test], [y[1] for y in test], color=\"tab:Purple\", marker=\"x\",label=\"Test\", s=1000)\n",
    "    \n",
    "\n",
    "plt.legend(scatterpoints=1)\n",
    "\n",
    "plt.title('Two Moons Dataset')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **SciKit Learn**, a very very popular python Machine Learning library for most classification tasks. Check out the [website and the documentation](https://scikit-learn.org/stable/).\n",
    "\n",
    "![](scikit-learn-logo.png)\n",
    "\n",
    "Here, we will use the [`KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Here we initialize the model. The key parameter is the number of neighbors (k) to consider. \n",
    "model = KNeighborsClassifier(n_neighbors = 100)\n",
    "# Here we train the model with the data (X) and the labels (y)\n",
    "model.fit(X, y)\n",
    "\n",
    "# We can then use new, never before seen data to make predictions\n",
    "print(test1, \"expected 1, predicted:\", model.predict([test1]))\n",
    "print(test1, \"expected 1, predicted:\", model.predict([test1]))\n",
    "print(test2, \"expected 0, predicted:\", model.predict([test2]))\n",
    "print(test3, \"borderline 1, predicted:\", model.predict([test3]))\n",
    "print(test4, \"borderline 0, predicted:\", model.predict([test4]))\n",
    "print(test5, \"borderline 0, predicted:\", model.predict([test5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out the above code with different levels of k, in particular very low k-values of 1 or 2. \n",
    "\n",
    "We can also plot **decision boundaries** for classifiers, which is a very intuitive method in 2D to understand what's going on: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 100)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot the data, color by class\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"SteelBlue\", marker=\"s\",label=\"0\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"OrangeRed\", marker=\"o\",label=\"1\")\n",
    "plt.legend(scatterpoints=1)\n",
    "\n",
    "\n",
    "# Computing and plotting the decision boundaries\n",
    "padding = 0.2\n",
    "x_min, x_max = X[:,0].min() - padding, X[:, 0].max() + padding\n",
    "y_min, y_max = X[:,1].min() - padding, X[:, 1].max() + padding\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))\n",
    "zz = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, zz, cmap=matplotlib.colors.ListedColormap(['OrangeRed', 'SteelBlue']), alpha=.2)\n",
    "plt.contour(xx, yy, zz, colors=\"black\", alpha=1, linewidths=0.2) \n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.title('Classification of Two Moons using k-NN')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, try this out for different `k` values. Note that the larger the `k` values the smoother / simpler the decision boundary becomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Evaluating a classification method\n",
    "\n",
    "Recall that in regression methods, we had several methods for evaluating a particular model: \n",
    "- $R^2$ value\n",
    "- hypothesis tests associated with the model and individual predictor variables\n",
    "\n",
    "How can we evaluate the performance of a classification method? \n",
    "\n",
    "**Confusion Matrix**. The [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) is a table where each column of the matrix represents the number of samples in each predicted class and each row represents the number of samples in each actual class. \n",
    "\n",
    "Consider the results from a classifier that is trying to classify 27 images of cats, dogs, and rabbits. Here is an example of what the confusion matrix might look like\n",
    "\n",
    "<img src=\"ConfusionMatrix.png\" title=\"https://en.wikipedia.org/wiki/Confusion_matrix\" width=\"35%\">\n",
    "\n",
    "This classifier is very good at distinguishing between cats and rabbits but lousy at recognizing dogs...half are misclassified!\n",
    "\n",
    "\n",
    "**Precision vs. Recall.** Two key metrics that can be obtained from the confusion matrix for binary classification are [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall). \n",
    "\n",
    "Precision measures how accurately our predicted set contains only the desired class, i.e., a high false positive rate leads to low precision. \n",
    "\n",
    "Recall measures whether we accurately pick up all the cases in the desired class, i.e., a high false negative rate leads to low recall. \n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\textrm{precision} &= \\frac{\\textrm{TP}}{\\textrm{TP} + \\textrm{FP}}\\\\\n",
    "\\textrm{recall} &= \\frac{\\textrm{TP}}{\\textrm{TP} + \\textrm{FN}}\n",
    "\\end{align*}\n",
    "\n",
    "Here, TP is the number of true positives, FP is the number of false positives, and FN is the number of false negatives.\n",
    "\n",
    "Often, precision and recall are inversely related; it is not possible to increase one without decreasing the other. For example, a trivial way to have perfect precision (precision = 1 = 100%) is if you make one correct positive prediction. However, this is not very useful since there will be many false negatives, so recall will be low. Conversely, if you can make one correct negative prediction, recall will be 100%, while precision will be very low.  \n",
    "\n",
    "In a particular application, it might be more desirable to have either better precision or recall. \n",
    "\n",
    "<img src=\"BinaryConfusinoMatrix.png\" title=\"https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62\" width=\"55%\">\n",
    "\n",
    "There have been some attempts to combine precision and recall into a single measure, such as the [F-measure](https://en.wikipedia.org/wiki/Precision_and_recall#F-measure), which is the harmonic mean of precision and recall:\n",
    "$$\n",
    "\\textrm{F-measure} = 2\\frac{ \\textrm{precision} \\cdot \\textrm{recall} }{\\textrm{precision} + \\textrm{recall}}.\n",
    "$$\n",
    "The F-measure is large if precision and recall are close. \n",
    "\n",
    "The [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) further illustrates the trade-off between precision and recall. \n",
    "\n",
    "\n",
    "The [Jaccard similarity score](https://en.wikipedia.org/wiki/Jaccard_index) is another measure of accuracy, given by \n",
    "$$\n",
    "\\textrm{J} = \\frac{\\textrm{TP}}{\\textrm{TP} + \\textrm{FP} + \\textrm{TN}}.\n",
    "$$\n",
    "\n",
    "SciKit Learn provides these metrics for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "print('Confusion Matrix:')\n",
    "print(metrics.confusion_matrix(y_true = y, y_pred = y_pred))\n",
    "\n",
    "print('Precision = ', metrics.precision_score(y_true = y, y_pred = y_pred))\n",
    "print('Recall = ', metrics.recall_score(y_true = y, y_pred = y_pred))\n",
    "print('F-score = ', metrics.f1_score(y_true = y, y_pred = y_pred))\n",
    "print('Jaccard similarity score', metrics.jaccard_score(y_true = y, y_pred = y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset: The Iris dataset\n",
    "\n",
    "Let's try kNN on a real dataset. We'll use the super-common Iris dataset for a demonstration. This dataset was introduced in 1936 by the statistician [Sir Ronald A. Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher). \n",
    "\n",
    "The dataset contains 4 features (attributes) of 50 samples containing 3 different species of [iris plants](https://en.wikipedia.org/wiki/Iris_(plant)). The goal is to classify the species of iris plant given the attributes. \n",
    "\n",
    "**The species, or classes are:**\n",
    "1. Iris Setosa \n",
    "2. Iris Versicolour \n",
    "3. Iris Virginica\n",
    "\n",
    "**The features (attributes) we have are::**\n",
    "1. sepal length (cm) \n",
    "2. sepal width (cm) \n",
    "3. petal length (cm) \n",
    "4. petal width (cm) \n",
    "\n",
    "<img src=\"iris.png\" title=\"http://mirlab.org/jang/books/dcpr/dataSetIris.asp?title=2-2%20Iris%20Dataset\" width=\"20%\">\n",
    "\n",
    "\n",
    "The Iris dataset is available in many sources; we'll load it from seaborn where it's one of the demo datasets and look at the data using a scatterplot matrix first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "df = sns.load_dataset(\"iris\") # built-in dataset in seaborn \n",
    "print(df.describe())\n",
    "sns.pairplot(df, hue=\"species\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import data, scikit-learn also has this dataset built-in\n",
    "iris = load_iris()\n",
    "\n",
    "# For easy plotting and interpretation, we only use first 2 features here. \n",
    "# We're throwing away useful information - don't do this at home! \n",
    "X = iris.data[:,:2]  \n",
    "y = iris.target\n",
    "\n",
    "# Create color maps\n",
    "clrs = sns.color_palette(\"tab10\").as_hex();\n",
    "print(clrs)\n",
    "cmap_bold = matplotlib.colors.ListedColormap(clrs[:3])\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y,  marker=\"o\", cmap=cmap_bold, s=30)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - padding, X[:, 0].max() + padding\n",
    "y_min, y_max = X[:, 1].min() - padding, X[:, 1].max() + padding\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)    \n",
    "plt.title('Iris dataset')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We see that it would be fairly easy to separate the \"blue\" irises from the two classes. However, separating the \"green\" and \"orange\" ones would be a challenge. \n",
    "\n",
    "There are three classes, so we can't apply logistic regression. (This isn't completely true; there are extensions of logistic regression to handle more classes, but these are not very popular.) Instead we'll use k-NN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## set up the model, k-NN classification with k = ?  \n",
    "k = 20\n",
    "clf = KNeighborsClassifier(n_neighbors=k)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# plot classification \n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))\n",
    "zz = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, zz, cmap=cmap_bold, alpha=0.05, shading='gouraud')\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=30)\n",
    "\n",
    "plt.title('Classification of Iris dataset using k-NN with k = '+ str(k))\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix:')\n",
    "y_pred = clf.predict(X)\n",
    "print(metrics.accuracy_score(y_true=y, y_pred=y_pred))\n",
    "print(metrics.confusion_matrix(y_true = y, y_pred = y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've only done this with two dimensions, what happens if we do it with all of the dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 20\n",
    "clf = KNeighborsClassifier(n_neighbors=k)\n",
    "clf.fit(iris.data, y)\n",
    "\n",
    "y_pred = clf.predict(iris.data)\n",
    "print(metrics.accuracy_score(y_true=y, y_pred=y_pred))\n",
    "print(metrics.confusion_matrix(y_true=y, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Some preliminary comments on the parameter, $k$:** \n",
    "\n",
    "- For k large (say $k=100$), the *decision boundary* (boundary between classes) is smooth. The model is not very complex - it could basically be described by a few lines. The model has low variance in the sense that if the data were to change slightly, the model wouldn't change much. (There are many voters.) Since the model doesn't depend on the data very much, we might expect that it would *generalize* to new data points. \n",
    "\n",
    "- For k small (say $k=1$), the decision boundary is very wiggly. The model is very complex - it definitely can't be described by a few lines. The model has high variance in the sense that if the data were to change slightly, the model would change quite a bit. Since the model is very dependent on the dataset, we would say that it wouldn't generalize to new data points well. In this case, we would say that the model has overfit the data. (We saw a similar phenomena in regression using high degree polynomials.) \n",
    "\n",
    "\n",
    "**Questions:**\n",
    "1. How to choose k? (more on this below)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model generalizability and cross-validation\n",
    "\n",
    "In classification, and other prediction problems, we would like to develop a model on a dataset, the *training dataset*, that will not only perform well on that dataset but on similar data that the model hasn't yet seen, the *testing dataset*. If a model satisfies this criterion, we say that it is *generalizable*. \n",
    "\n",
    "If a model has 100% accuracy on the training dataset ($k=1$ in k-NN) but doesn't generalize to new data, then it isn't a very good model. We say that this model has *overfit* the data. On the other hand, it isn't difficult to see that we could also *underfit* the data (taking $k$ large in k-NN). In this case, the model isn't complex enough to have good accuracy on the training dataset. \n",
    "\n",
    "**Cross-validation** is a general method for assessing how the results of a predictive model (classification, regression,...) will *generalize* to an independent data set. In classification, cross-validation is a method for assessing how well the classification model will predict the class of points that weren't used to *train* the model. \n",
    "\n",
    "The idea of the method is simple: \n",
    "1. Split the dataset into two groups: the training dataset and the test dataset. \n",
    "2. Train the model on the training dataset. \n",
    "3. Check the accuracy of the model on the test dataset. \n",
    "\n",
    "In practice, you have to decide how to split the data into groups (i.e. how large the groups should be). You might also want to repeat the experiment so that the assessment doesn't depend on the way in which you split the data into groups. We'll worry about this in a later lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "X,y = make_moons(n_samples=1000,random_state=1,noise=0.5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2, test_size=0.7)\n",
    "\n",
    "for k in [1, 2, 5, 10, 50, 100]:\n",
    "    print(\"k is:\", k)\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy Train:\", metrics.accuracy_score(y_true=y_train, y_pred=clf.predict(X_train)))\n",
    "    print(\"Accuracy Test:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "    print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For now, I just want you to conceptually understand how generalizable k-NN is as we vary the parameter, k. \n",
    "\n",
    "<img src=\"BiasVarianceTradeoff.png\" width=\"500\">\n",
    "\n",
    "$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad$ \n",
    "source: [this blog](https://blog.cambridgecoding.com/2016/03/24/misleading-modelling-overfitting-cross-validation-and-the-bias-variance-trade-off/)\n",
    "\n",
    "As the model becomes more complex (k decreases), the accuracy always increases for the training dataset. But, at some point, it starts to overfit the data and the accuracy decreases for the test dataset! Cross validation techniques will allow us to find the sweet-spot for the parameter k! (Think: Goldilocks and the Three Bears.)\n",
    "\n",
    "\n",
    "Let's look at the decision boundaries for the two moons dataset. You can use the *train_test_split* function in scikit-learn to split the dataset into a training dataset and a test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_plot_dimension(X, h=0.02, b=0.05):\n",
    "    x_min, x_max = X[:, 0].min() - b, X[:, 0].max() + b\n",
    "    y_min, y_max = X[:, 1].min() - b, X[:, 1].max() + b\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    dimension = xx, yy\n",
    "    return dimension\n",
    " \n",
    "def detect_decision_boundary(dimension, model):\n",
    "    xx, yy = dimension  # unpack the dimensions\n",
    "    boundary = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    boundary = boundary.reshape(xx.shape)  # Put the result into a color plot\n",
    "    return boundary\n",
    " \n",
    "def plot_decision_boundary(panel, dimension, boundary, colors=['#DADDED', '#FBD8D8']):\n",
    "    xx, yy = dimension  # unpack the dimensions\n",
    "    panel.contourf(xx, yy, boundary, cmap=matplotlib.colors.ListedColormap(colors), alpha=1)\n",
    "    panel.contour(xx, yy, boundary, colors=\"g\", alpha=1, linewidths=0.5)  # the decision boundary in green\n",
    " \n",
    "def plot_dataset(panel, X, y, colors=[\"#EE3D34\", \"#4458A7\"], markers=[\"x\", \"o\"]):\n",
    "    panel.scatter(X[y == 1, 0], X[y == 1, 1], color=colors[0], marker=markers[0])\n",
    "    panel.scatter(X[y == 0, 0], X[y == 0, 1], color=colors[1], marker=markers[1])\n",
    " \n",
    "def calculate_prediction_error(model, X, y):\n",
    "    yPred = model.predict(X)\n",
    "    score = round(metrics.accuracy_score(y, yPred), 2)\n",
    "    return score\n",
    " \n",
    "def plot_prediction_error(panel, dimension, score, b=.3):\n",
    "    xx, yy = dimension  # unpack the dimensions\n",
    "    panel.text(xx.max() - b, yy.min() + b, ('%.2f' % score).lstrip('0'), size=15, horizontalalignment='right')\n",
    " \n",
    "def explore_fitting_boundaries(model, n_neighbors, datasets, width):  \n",
    "    # determine the height of the plot given the aspect ration of each panel should be equal\n",
    "    height = float(width)/len(n_neighbors) * len(datasets.keys())\n",
    " \n",
    "    nrows = len(datasets.keys())\n",
    "    ncols = len(n_neighbors)\n",
    " \n",
    "    # set up the plot\n",
    "    figure, axes = plt.subplots(\n",
    "        nrows,\n",
    "        ncols,\n",
    "        figsize=(width, height),\n",
    "        sharex=True,\n",
    "        sharey=True\n",
    "    )\n",
    " \n",
    "    dimension = detect_plot_dimension(X, h=0.02)  # the dimension each subplot based on the data\n",
    " \n",
    "    # Plotting the dataset and decision boundaries\n",
    "    i = 0\n",
    "    for n in n_neighbors:\n",
    "        model.n_neighbors = n\n",
    "        model.fit(datasets[\"Training Set\"][0], datasets[\"Training Set\"][1])\n",
    "        boundary = detect_decision_boundary(dimension, model)\n",
    "        j = 0\n",
    "        for d in datasets.keys():\n",
    "            try:\n",
    "                panel = axes[j, i]\n",
    "            except (TypeError, IndexError):\n",
    "                if (nrows * ncols) == 1:\n",
    "                    panel = axes\n",
    "                elif nrows == 1:  # if you only have one dataset\n",
    "                    panel = axes[i]\n",
    "                elif ncols == 1:  # if you only try one number of neighbors\n",
    "                    panel = axes[j]\n",
    "            plot_decision_boundary(panel, dimension, boundary)  # plot the decision boundary\n",
    "            plot_dataset(panel, X=datasets[d][0], y=datasets[d][1])  # plot the observations\n",
    "            score = calculate_prediction_error(model, X=datasets[d][0], y=datasets[d][1])\n",
    "            plot_prediction_error(panel, dimension, score, b=0.2)  # plot the score\n",
    " \n",
    "            # make compacted layout\n",
    "            panel.set_frame_on(False)\n",
    "            panel.set_xticks([])\n",
    "            panel.set_yticks([])\n",
    " \n",
    "            # format the axis labels\n",
    "            if i == 0:\n",
    "                panel.set_ylabel(d)\n",
    "            if j == 0:\n",
    "                panel.set_title('k={}'.format(n))\n",
    "            j += 1\n",
    "        i += 1   \n",
    " \n",
    "    plt.subplots_adjust(hspace=0, wspace=0)  # make compacted layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y, random_state=1, test_size=0.5)\n",
    "\n",
    "# specify the model and settings\n",
    "model = KNeighborsClassifier()\n",
    "n_neighbors = [200, 99, 50, 23, 11, 1]\n",
    "datasets = {\n",
    "    \"Training Set\": [XTrain, yTrain],\n",
    "    \"Test Set\": [XTest, yTest]\n",
    "}\n",
    "width = 20\n",
    "\n",
    "explore_fitting_boundaries(model=model, n_neighbors=n_neighbors, datasets=datasets, width=width)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions and take-away\n",
    "\n",
    "1. k-NN is a very simple method that can be used for classification. (It can be used for regression too! How?)  \n",
    "2. Model accuracy (measured on the training dataset) and generalizability (measured on the testing dataset) are both important and often in contention with one another. \n",
    "3. Model accuracy can be measured using the confusion matrix, precision, recall, F-measure, or the Jaccard similarity score. Generalizability can be measured via cross validation. \n",
    "4. Picking parameters in models (such as k in k-NN) is non-trivial, but can be done via cross validation. \n",
    "\n",
    "\n",
    "### Classification method preview\n",
    "For a quick preview of other classification methods, see the comparison [here](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html).\n",
    "\n",
    "![](http://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
