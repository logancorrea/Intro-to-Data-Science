{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Data Science \n",
    "# Lecture 21: Neural Networks I\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "In this lecture, we'll discuss Neural Networks, which can be used for both Classification and Regression. In particular, we'll discuss \n",
    "* perceptrons and multi-layer perceptrons (MLP)\n",
    "* neural networks with scikit-learn\n",
    "* how to train a neural network\n",
    "\n",
    "Recommended reading:\n",
    "* A. Géron, [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlowTensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/) (2022), Ch. 10-13. See also the [associated github page](https://github.com/ageron/handson-ml3). \n",
    "* I. Goodfellow and Y. Bengio and A. Courville, [Deep Learning](http://www.deeplearningbook.org/)\n",
    "* [TensorFlow tutorials](https://www.tensorflow.org/tutorials)\n",
    "\n",
    "Recommended watching:\n",
    "* Welch Labs, [Neural Networks demystified videos](https://www.youtube.com/watch?v=bxe2T-V8XRs&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU) and [accompanying code](https://github.com/stephencwelch/Neural-Networks-Demystified) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_iris, make_moons, load_breast_cancer, fetch_california_housing\n",
    "from sklearn.datasets import get_data_home\n",
    "# from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import io #will use this to load matlab data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15,9)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks and deep learning\n",
    "\n",
    "[Artificial Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network) were originally motivated by the brain, which is composed of a network of neurons. Each neuron receives a (chemical) signal from other neurons, does a small computation and then decides if and how to release more chemicals. This composition of small calculations can perform complicated tasks! Similarly, an artificial neural network is a network composed of neurons, which we simply think of as computational units. Although artificial neural networks were originally motivated by the brain, modern artificial neural networks *do not* try to model it. \n",
    "\n",
    "Neural Networks can generally be used for supervised learning tasks, such as classification and regression. (Neural Networks can also be used for unsupervised learning and reinforcement learning, but we won't talk about this.) So, after today, we'll have the following methods for these two supervised learning tasks: \n",
    "\n",
    "**Regression** \n",
    "+ Linear regression \n",
    "+ k Nearest Neighbors (k-NN)\n",
    "+ Decision Trees\n",
    "+ *Neural Networks*\n",
    "\n",
    "**Classification**\n",
    "+ k Nearest Neighbors (k-NN)\n",
    "+ Decision Trees\n",
    "+ Support vector machines (SVM)\n",
    "+ *Neural Networks*\n",
    "\n",
    "Large-scale neural networks are at the core of [deep learning](https://en.wikipedia.org/wiki/Deep_learning), which has gained much publicity for performing very impressive machine learning tasks in the past few years, such as, \n",
    "* classifying billions of images (*e.g.*, Google Images)\n",
    "+ speech recognition (*e.g.*, Amazon's Alexa or Apple’s Siri)\n",
    "+ video recommendation (*e.g.*, YouTube) \n",
    "+ beating the world champion at the game of Go ([DeepMind’s AlphaGo](https://en.wikipedia.org/wiki/AlphaGo))\n",
    "+ answering questions [ChatGPT](https://openai.com/blog/chatgpt)\n",
    "+ scientific discovery\n",
    "\n",
    "As an example, check out the [Open Images Challenge]( https://storage.googleapis.com/openimages/web/index.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perceptrons\n",
    "\n",
    "The simplest neural network is called the [perceptron](https://en.wikipedia.org/wiki/Perceptron) and was invented in 1957 by Frank Rosenblatt. The *perceptron* is a binary classifier. It is a function that maps a set of $n$ real features $X = \\{x_1, x_2, ..., x_n\\}$ to a binary output: \n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "1 & \\textrm{if } \\ w\\cdot x + b > 0 \\\\\n",
    "0 & \\textrm{otherwise}\n",
    "\\end{cases}. \n",
    "$$\n",
    "Here, $w \\in \\mathbb R^n$ is a vector of weights and $b$ is a scalar called the *bias*. (This is very similar to the binary classifier we saw when looking at support vector machines.) \n",
    "\n",
    "It is customary to represent this function by the following diagram. \n",
    "\n",
    "<img src=\"perceptron.png\" title=\"https://tex.stackexchange.com/questions/104334/tikz-diagram-of-a-perceptron\" width=\"400\">\n",
    "\n",
    "Just like for previous classification methods, we first *train* the network on data, which is to say that we find good choices of $w$ and $b$ for the training dataset. Then we can see how well the neural network performs on the test dataset or use it to classify new data points.\n",
    "\n",
    "Of course, a single perceptron is only a linear discriminator (similar to logistic regression and linear SVM). But things become much more interesting when you start composing many neurons, that is, considering networks with more *layers*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron\n",
    "\n",
    "Like the percpetron, a [**multi-layer perceptron**](https://en.wikipedia.org/wiki/Multilayer_perceptron) is a function that maps features $X = \\{x_1, x_2, ..., x_n\\}$ to a target $y$. Here is a diagram of a perceptron with three *layers*. \n",
    "\n",
    "\n",
    "<img src=\"Colored_neural_network.svg\" title=\"https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Colored_neural_network.svg\" \n",
    "width=\"300\">\n",
    " \n",
    "In this neural network, we have \n",
    "+ an *input layer*, which is the same size as the number of features, \n",
    "+ a middle layer, which is usually called a *hidden layer* \n",
    "+ and an *output layer*, which should match the target variables\n",
    "\n",
    "Each layer in this diagram represents a function\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "x^{h}_1 \\\\\n",
    "x^{h}_2 \\\\\n",
    "x^{h}_3 \\\\\n",
    "x^{h}_4\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\sigma \\left(\n",
    "\\begin{pmatrix}\n",
    "A^h_{1,1} & A^h_{1,2} & A^h_{1,3} \\\\\n",
    "A^h_{2,1} & A^h_{2,2} & A^h_{2,3} \\\\\n",
    "A^h_{3,1} & A^h_{3,2} & A^h_{3,3} \\\\\n",
    "A^h_{4,1} & A^h_{4,2} & A^h_{4,3}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x^{i}_1 \\\\\n",
    "x^{i}_2 \\\\\n",
    "x^{i}_3\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "b^{h}_1 \\\\\n",
    "b^{h}_2 \\\\\n",
    "b^{h}_3 \\\\\n",
    "b^{h}_4\n",
    "\\end{pmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "x^{o}_1 \\\\\n",
    "x^{o}_2\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\sigma \\left(\n",
    "\\begin{pmatrix}\n",
    "A^o_{1,1} & A^o_{1,2} & A^o_{1,3} & A^o_{1,4}  \\\\\n",
    "A^o_{2,1} & A^o_{2,2} & A^o_{2,3} & A^o_{1,4}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x^{h}_1 \\\\\n",
    "x^{h}_2 \\\\\n",
    "x^{h}_3 \\\\\n",
    "x^{h}_4 \n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "b^{o}_1 \\\\\n",
    "b^{o}_2\n",
    "\\end{pmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Here, the function $\\sigma$ is called the [**activation function**](https://en.wikipedia.org/wiki/Activation_function). We'll discuss these later. \n",
    "\n",
    "An additional function, *e.g.* [softmax](https://en.wikipedia.org/wiki/Softmax_function), is applied to the output layer to give the prediction for classification problems. Softmax converts numerical values to probabilities.\n",
    "\n",
    "Training the network now means that we find the best matrices $A^h$ and $A^o$ as well as the best biases, $b^h$ and $b^o$ for the data. For this relatively simple network, this is already \n",
    "$$ \n",
    "26 = 3\\times 4 + 4\\times 2 + 4 + 2 \n",
    "\\textrm{ parameters}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other network architectures \n",
    "\n",
    "The way in which we put together the neurons is referred to as the **network architecture**. The perceptron and multilayer perceptron are two examples. There are many more. Here is a peek at the [neural-network-zoo](http://www.asimovinstitute.org/neural-network-zoo/): \n",
    "<img src=\"neuralnetworks.png\" title=\"http://www.asimovinstitute.org/neural-network-zoo/\" \n",
    "width=\"700\">\n",
    "\n",
    "The most important thing about all of these neural networks is that there is an *input layer*, typically drawn on the left hand side and an *output layer*, typically drawn on the right hand side. The middle layers are called *hidden layers*. \n",
    "\n",
    "A *deep neural network* is simply a neural network with many hidden layers. \n",
    "\n",
    "In all of these neural network designs, each layer has its own weight matrix and bias vector that needs to be trained (learned). Consequently, *training* a neural network is a much harder job than we have seen for previous methods. It also requires more data. \n",
    "\n",
    "**Example: ImageNet** The [ImageNet project](https://en.wikipedia.org/wiki/ImageNet) is a test dataset for visual object recognition (classification task). The dataset consists of \n",
    "+ 14 million images that have been hand-annotated \n",
    "+ there are approximately 20,000 categories with a typical category, such as \"balloon\" or \"strawberry\", consisting of several hundred images.\n",
    "\n",
    "<img src=\"ImageNetPlot.png\" title=\"https://qz.com/1046350/the-quartz-guide-to-artificial-intelligence-what-is-it-why-is-it-important-and-should-we-be-afraid/\" \n",
    "width=\"500\">\n",
    "\n",
    "The 2017 winning solution, described in [this paper](https://arxiv.org/abs/1709.01507), used a deep Neural Network with 154 layers and $\\sim 10^8$ parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks with scikit-learn\n",
    "\n",
    "Scikit-learn has a Neural Network library [here](http://scikit-learn.org/stable/modules/neural_networks_supervised.html). \n",
    "\n",
    "I want to introduce NN uisng scikit-learn, but wouldn't use this library in practice. Here's a warning from the library: \n",
    "```\n",
    "Warning This implementation is not intended for large-scale applications. In particular, scikit-learn offers no GPU support. For much faster, GPU-based implementations, as well as frameworks offering much more flexibility to build deep learning architectures, see Related Projects.\n",
    "```\n",
    "\n",
    "Scikit-learn has a few different neural network functions:\n",
    "1. [perceptron](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)\n",
    "+ [multi-layer perceptron (MLP) classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
    "+ [multi-layer perceptron (MLP) regressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)\n",
    "\n",
    "Let's first test the `MLPClassifier` on the [two moons dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X contains two features\n",
    "# y contains labels\n",
    "X,y = make_moons(n_samples=1000,random_state=1,noise=0.2)\n",
    "X = StandardScaler().fit_transform(X) #makes each feature have mean zero and unit variance\n",
    "\n",
    "# Plot the data, color by class\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"DarkBlue\", marker=\"s\",label=\"class 1\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"DarkRed\", marker=\"o\",label=\"class 2\")\n",
    "plt.legend(scatterpoints=1)\n",
    "plt.title('Two Moons Dataset')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(3,3,3), max_iter=1000, alpha=1e-4,\n",
    "                    solver='adam', verbose=True, random_state=1, \n",
    "                    learning_rate_init=.1)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the data, color by class\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"DarkBlue\", marker=\"s\",label=\"class 1\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"DarkRed\", marker=\"o\",label=\"class 2\")\n",
    "plt.legend(scatterpoints=1)\n",
    "\n",
    "# Plot the predictions made by NN\n",
    "x_min, x_max = X[:,0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))\n",
    "zz = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, zz, cmap=ListedColormap(['DarkRed', 'DarkBlue']), alpha=.2)\n",
    "plt.contour(xx, yy, zz, colors=\"black\", alpha=1, linewidths=0.2) \n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.title('Classification of Two Moons using MLPClassifier')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('After ', model.n_iter_, ' iterations, the loss is ', model.loss_)\n",
    "print('model coef shapes')\n",
    "[print(coef.shape) for coef in model.coefs_]\n",
    "print('model coefs')\n",
    "[print(coef) for coef in model.coefs_]\n",
    "print('model intercepts')\n",
    "[print(coef) for coef in model.intercepts_]\n",
    "\n",
    "print(model.get_params())\n",
    "\n",
    "# For binary classification, the activation function for output layer is logistic function:\n",
    "print(model.out_activation_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "There are a lot more function parameters for [`MLPClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) than for other scikit-learn classification methods. You'll find that tweaking them also makes a very big difference in the output. Here are some of the important parameters:\n",
    "\n",
    "#### Network architecture parameters\n",
    "\n",
    "+ **hidden_layer_sizes**: tuple, length = n_layers - 2, default (100,). \n",
    "The i-th element represents the number of neurons in the i-th hidden layer.\n",
    "\n",
    "+ **activation**: {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’. \n",
    "Activation function for the hidden layer.\n",
    " - ‘identity’, no-op activation, useful to implement linear bottleneck, returns $\\sigma(x) = x$\n",
    " - ‘logistic’, the logistic sigmoid function, returns $\\sigma(x) = 1 / (1 + exp(-x)$).\n",
    " - ‘tanh’, the hyperbolic tan function, returns $\\sigma(x) = tanh(x)$.\n",
    " - ‘relu’, the rectified linear unit function, returns $\\sigma(x) = max(0, x)$.\n",
    " \n",
    "+ **alpha**: float, optional, default 0.0001. L2 penalty (regularization term) parameter. See the effect of alpha [here](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html).\n",
    "\n",
    "#### Optimization related parameters\n",
    "+ **solver**: {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’. \n",
    "The solver for parameter optimization.\n",
    " - ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n",
    " - ‘sgd’ refers to stochastic gradient descent.\n",
    " - ‘adam’ refers to another stochastic gradient-based optimizer\n",
    "\n",
    "+ **max_iter**: int, optional, default 200. \n",
    "Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations. For stochastic solvers (‘sgd’, ‘adam’), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.\n",
    "\n",
    "+ **random_state**: int, RandomState instance or None, optional, default None. \n",
    "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "\n",
    "+ **tol**: float, optional, default 1e-4. \n",
    "Tolerance for the optimization. When the loss or score is not improving by at least tol for two consecutive iterations, unless learning_rate is set to ‘adaptive’, convergence is considered to be reached and training stops.\n",
    "\n",
    "+ **verbose**: bool, optional, default False. Whether to print progress messages to stdout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation function \n",
    "\n",
    "There are several choices of **activation function**, $\\sigma(x)$: hyperbolic tangent, logistic, and rectified linear unit (ReLU). In the previous example, we used the default activation function, ReLU. \n",
    "\n",
    " Many more activations functions are listed on [this wikipedia page](https://en.wikipedia.org/wiki/Activation_function).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see Géron, Ch. 10\n",
    "\n",
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def derivative(f, z, eps=0.000001):\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)\n",
    "\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(z, np.sign(z), \"r-\", linewidth=2, label=\"Step\")\n",
    "plt.plot(z, logit(z), \"g--\", linewidth=2, label=\"Logit\")\n",
    "plt.plot(z, np.tanh(z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Activation functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 1.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=2, label=\"Step\")\n",
    "plt.plot(0, 0, \"ro\", markersize=5)\n",
    "plt.plot(z, derivative(logit, z), \"g--\", linewidth=2, label=\"Logit\")\n",
    "plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, derivative(relu, z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.plot(0, 0, \"mx\", markersize=10)\n",
    "plt.grid(True)\n",
    "#plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Derivatives\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Exercise**: By changing the hidden_layer_sizes and activation function, can you find a better classifier for the two moons dataset above?\n",
    "\n",
    "You might want to remove the preselected random_state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Training a neural network\n",
    "We'll follow the scikit-learn  [user guide](http://scikit-learn.org/stable/modules/neural_networks_supervised.html) \n",
    "to see how the multi-layer perceptron (MLP) neural network is trained. \n",
    "\n",
    "The MLP uses a loss function of the form \n",
    "$$\n",
    "Loss(\\hat{y},y,W) =  \\frac{1}{2} \\sum_{i=1}^n f(\\hat{y}_i(x_i,W),y_i) + \\frac{\\alpha}{2} \\|W\\|_2^2\n",
    "$$\n",
    "Here, \n",
    "+ $y_i$ are the true labels for the $i$-th example, \n",
    "+ $\\hat{y}_i(x_i,W)$ are the predicted label for the $i$-th example, \n",
    "+ W is a list of all the parameters in the model\n",
    "+ $f$ is a function that measures the error, typically $L^2$ difference for regression or cross-entropy for classification, and \n",
    "+ $\\alpha$ is a regularization parameter. \n",
    "\n",
    "Starting from initial random weights, the loss function is minimized by repeatedly updating these weights. The details of this depend on the chosen method, either a quasi-Newton method `lbfgs`, stochastic gradient descent `sgd`, or `adam`. \n",
    "\n",
    "In the **gradient descent method**, the gradient $\\nabla_{W} Loss$ of the loss with respect to the weights is computed. The weights are then changed in the negative gradient direction using a step-length or learning-rate, $\\varepsilon>0$: \n",
    "$$\n",
    "W \\leftarrow W - \\varepsilon \\nabla_W {Loss}.\n",
    "$$\n",
    "The algorithm stops when it reaches a preset maximum number of iterations, `max_iter`, \n",
    "or when the improvement in loss is below a preset small number, `tol`.\n",
    "\n",
    "The gradient of $W$ is simply computed using the chain rule from calculus. In principle the idea is simple, but in practice it is a tedious and complicated computation. Data analysts have figured out a clever way how to organize this calculation, which is called **back propagation**. \n",
    "\n",
    "A complete description of `lbfgs`, `sgd`, and `adam` is beyond the scope of the course. I'll just say that they are clever modifications to the gradient descent method. \n",
    "\n",
    "Let's see a comparison of optimization methods, taken from [this page](http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# different learning rate schedules and momentum parameters\n",
    "params = [{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0,\n",
    "           'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,\n",
    "           'nesterovs_momentum': False, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,\n",
    "           'nesterovs_momentum': True, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0,\n",
    "           'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,\n",
    "           'nesterovs_momentum': True, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,\n",
    "           'nesterovs_momentum': False, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'adam', 'learning_rate_init': 0.01}]\n",
    "\n",
    "labels = [\"constant learning-rate\", \"constant with momentum\",\n",
    "          \"constant with Nesterov's momentum\",\n",
    "          \"inv-scaling learning-rate\", \"inv-scaling with momentum\",\n",
    "          \"inv-scaling with Nesterov's momentum\", \"adam\"]\n",
    "\n",
    "plot_args = [{'c': 'red', 'linestyle': '-'},\n",
    "             {'c': 'green', 'linestyle': '-'},\n",
    "             {'c': 'blue', 'linestyle': '-'},\n",
    "             {'c': 'red', 'linestyle': '--'},\n",
    "             {'c': 'green', 'linestyle': '--'},\n",
    "             {'c': 'blue', 'linestyle': '--'},\n",
    "             {'c': 'black', 'linestyle': '-'}]\n",
    "\n",
    "\n",
    "def plot_on_dataset(X, y, ax, name):\n",
    "    # for each dataset, plot learning for each learning strategy\n",
    "    print(\"\\nlearning on dataset %s\" % name)\n",
    "    ax.set_title(name)\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    mlps = []\n",
    "    max_iter = 5000\n",
    "\n",
    "    for label, param in zip(labels, params):\n",
    "        print(\"training: %s\" % label)\n",
    "        mlp = MLPClassifier(verbose=0, random_state=0,\n",
    "                            max_iter=max_iter, **param)\n",
    "        mlp.fit(X, y)\n",
    "        mlps.append(mlp)\n",
    "        print(\"Training set score: %f\" % mlp.score(X, y))\n",
    "        print(\"Training set loss: %f\" % mlp.loss_)\n",
    "    for mlp, label, args in zip(mlps, labels, plot_args):\n",
    "            ax.plot(mlp.loss_curve_, label=label, **args)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "# load / generate some toy datasets\n",
    "iris = datasets.load_iris()\n",
    "digits = datasets.load_digits()\n",
    "data_sets = [(iris.data, iris.target),\n",
    "             (digits.data, digits.target),\n",
    "             datasets.make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "             datasets.make_moons(noise=0.3, random_state=0)]\n",
    "\n",
    "for ax, data, name in zip(axes.ravel(), data_sets, ['iris', 'digits',\n",
    "                                                    'circles', 'moons']):\n",
    "    plot_on_dataset(*data, ax=ax, name=name)\n",
    "\n",
    "fig.legend(ax.get_lines(), labels, ncol=3, loc=\"upper center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Some advise on optimization methods according to [this page](http://scikit-learn.org/stable/modules/neural_networks_supervised.html): \n",
    "* Empirically, we observed that L-BFGS converges faster and with better solutions on small datasets. For relatively large datasets, however, Adam is very robust. It usually converges quickly and gives pretty good performance. SGD with momentum or Nesterov’s momentum, on the other hand, can perform better than those two algorithms if learning rate is correctly tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: breast cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "print(cancer.keys())\n",
    "\n",
    "# 569 data points with 30 features\n",
    "cancer['data'].shape\n",
    "\n",
    "# full description:\n",
    "print(cancer['DESCR'])\n",
    "\n",
    "X = cancer['data']\n",
    "y = cancer['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # Fit only to the training data\n",
    "\n",
    "# Apply scaling to data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30),random_state=1)\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "get_data_home()\n",
    "print(type(X_train))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: MNIST dataset\n",
    "\n",
    "Let's train a multi layer perceptron on the MNIST dataset. \n",
    "\n",
    "This MNIST database contains a total of 70,000 examples of handwritten digits of size 28x28 pixels, labeled from 0 to 9.\n",
    "\n",
    "Following the hints on [this website](https://stackoverflow.com/questions/51301570/fetch-mldata-how-to-manually-set-up-mnist-dataset-when-source-server-is-down/51301798#51301798), \n",
    "you the dataset was downloaded directly from [this website](https://github.com/amplab/datascience-sp14/blob/master/lab7/mldata/mnist-original.mat). A zipped version is in the lectures repository; you will need to unzip it before running the following code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mnist = io.loadmat('mnist-original.mat')\n",
    "print(type(mnist))\n",
    "print(mnist.keys())\n",
    "\n",
    "# rescale the data, use the traditional train/test split\n",
    "X, y = mnist['data'] / 255., mnist['label']\n",
    "X = np.transpose(X)\n",
    "y = np.ravel(y)\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n",
    "                     solver='adam', verbose=10, tol=1e-4, random_state=1)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('After ', mlp.n_iter_, ' iterations, the loss is ', mlp.loss_)\n",
    "print('model coef shapes')\n",
    "[print(coef.shape) for coef in mlp.coefs_]\n",
    "\n",
    "# For multiclass classification with k classes, the output is k-dim and output activation function is softmax:\n",
    "print(mlp.out_activation_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Exercise**: By adjusting the parameters in the MLPClassifier, improve the test set score. \n",
    "    \n",
    "\n",
    "**Note**: [This webpage](http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html)\n",
    "tries to interpret the MLP classification weights learned for the MNIST dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Regression with Neural Networks in scikit-learn\n",
    "\n",
    "We can also do regression using Neural Networks! We'll illustrate this on a California housing dataset. \n",
    "\n",
    "We'll use a multi-layer perceptron for regression, implemented in the scikit-learn [`MLPRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "print(housing.keys())\n",
    "\n",
    "# 20640 data points with 8 features\n",
    "print(housing['data'].shape)\n",
    "\n",
    "# full description:\n",
    "print(housing['DESCR'])\n",
    "\n",
    "X = housing['data']\n",
    "y = housing['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # Fit only to the training data\n",
    "\n",
    "# Apply scaling to data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Linear Regression with Scikit-Learn\n",
    "lin_reg = LinearRegression()\n",
    "print(lin_reg.get_params())\n",
    "\n",
    "lin_reg.fit(X_train, y_train)\n",
    "print(lin_reg.intercept_)\n",
    "print(lin_reg.coef_)\n",
    "\n",
    "print(lin_reg.score(X_test,y_test)) # score = 1 is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# MLP regression with Scikit-Learn\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=(8,8),verbose=0,random_state=2,solver='adam')\n",
    "print(mlp_reg.get_params())\n",
    "\n",
    "mlp_reg.fit(X_train, y_train)\n",
    "\n",
    "print(mlp_reg.score(X_test,y_test)) # score = 1 is good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Another resource: Neural Networks demystified\n",
    "\n",
    "There is a good sequence of 7 videos called **Neural Networks demystified** from Welch Labs that builds and trains a neural network from scratch in python. \n",
    "* Part 1: [Data + Architecture](https://www.youtube.com/watch?v=bxe2T-V8XRs)\n",
    "+ Part 2: [Forward Propagation](https://www.youtube.com/watch?v=UJwK6jAStmg)\n",
    "+ Part 3: [Gradient Descent](https://www.youtube.com/watch?v=5u0jaA3qAGk)\n",
    "+ Part 4: [Backpropagation](https://www.youtube.com/watch?v=GlcnxUlrtek)\n",
    "+ Part 5: [Numerical Gradient Checking](https://www.youtube.com/watch?v=pHMzNW8Agq4&t=22s)\n",
    "+ Part 6: [Training](https://www.youtube.com/watch?v=9KM9Td6RVgQ)\n",
    "+ Part 7: [Overfitting, Testing, and Regularization](https://www.youtube.com/watch?v=S4ZUwgesjS8)\n",
    "\n",
    "If you're interested in learning more about how Neural Networks are trained, I would recommend watching these videos. \n",
    "\n",
    "The accompanying code is [on github](https://github.com/stephencwelch/Neural-Networks-Demystified) and can be obtained via \n",
    "```\n",
    "git clone https://github.com/stephencwelch/Neural-Networks-Demystified.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks for large scale applications.\n",
    "\n",
    "We have now seen how to use neural networks in scikit-learn. However, this implementation does not scale to large-scale applications. (There is no GPU support and limited architectures.) \n",
    "\n",
    "There are many other packages that have more advanced implementations of neural networks. Here is a partial list with short descriptions taken from the packages.\n",
    "* [TensorFlow](https://github.com/tensorflow/tensorflow): TensorFlow™ is an open source C++ software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. \n",
    "+ [PyTorch](http://pytorch.org/): PyTorch is a deep learning framework that puts Python first. \n",
    "+ [CNTK](https://github.com/Microsoft/cntk)  Cognitive Toolkit (CNTK) is an open source deep-learning toolkit developed by Microsoft. \n",
    "+ [Theano](https://github.com/Theano/Theano): Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation.\n",
    "+ [Torch](http://torch.ch/): Torch is a scientific computing framework with wide support for machine learning algorithms that puts GPUs first.\n",
    "\n",
    "+ [keras](https://github.com/keras-team/keras): Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.\n",
    "+ [MXNet](https://github.com/dmlc/mxnet): Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Scala, Go, Javascript and more\n",
    "+ [Caffe](http://caffe.berkeleyvision.org/): Caffe is a deep learning framework made with expression, speed, and modularity in mind. It is developed by Berkeley AI Research (BAIR) and by community contributors.\n",
    "+ [Lasagne](https://github.com/Lasagne/Lasagne): Lightweight library to build and train neural networks in Theano. \n",
    "+ [prettytensor](https://github.com/google/prettytensor/): \n",
    "Pretty Tensor provides a high level builder API for TensorFlow. It provides thin wrappers on Tensors so that you can easily build multi-layer neural networks.\n",
    "\n",
    "+ [Deeplearning4j](https://deeplearning4j.org/): Open-Source, Distributed, Deep Learning Library for the JVM\n",
    "+ [H2O](https://github.com/h2oai): Fast Scalable Machine Learning For Smarter Applications\n",
    "+ ... \n",
    "\n",
    "\n",
    "## Neural Networks for large scale applications.\n",
    "We'll continue this discussion in **Lecture 27: Neural Networks and Tensor Flow**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
